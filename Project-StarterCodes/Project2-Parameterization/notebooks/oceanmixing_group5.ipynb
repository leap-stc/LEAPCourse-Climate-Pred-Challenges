{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-Based Parameterization of Vertical Mixing Coefficients in the Ocean Surface Boundary Layer\n",
    "\n",
    "This notebook extends the analysis carried out by **Sane et al. (2023)** \"Parameterizing vertical mixing coefficients in the ocean surface boundary layer using neural networks\" by implementing a **conditional modeling approach based on clustered ocean states**.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "The original paper demonstrated that neural networks can effectively predict vertical mixing shapes in the ocean. However, we hypothesize that **different ocean states might benefit from specialized models**. By clustering the input feature space, we can potentially develop models that are better tailored to specific oceanic conditions, further improving prediction accuracy.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Reproduce baseline results** from Sane et al. (2023)\n",
    "2. **Cluster ocean states** based on physical parameters\n",
    "3. **Train separate models** for each cluster\n",
    "4. **Compare performance** of the cluster-specific approach vs. the baseline\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- Identification of distinct oceanic regimes with different mixing behaviors\n",
    "- Improved prediction accuracy, especially in regions with complex dynamics\n",
    "- Better understanding of how different physical drivers influence mixing in different regimes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport copy\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport seaborn as sns\nimport pandas as pd\nfrom tqdm import tqdm\nimport xarray as xr\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom datetime import datetime\nimport warnings\nfrom matplotlib.gridspec import GridSpec\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Setup notebook environment\ntoday = datetime.today()\nnp.random.seed(100)\ntorch.manual_seed(100)\n\n# Fix paths\ncwd = os.getcwd()\nparent_dir = os.path.dirname(cwd)\nos.chdir(parent_dir)\ncwd = parent_dir\nprint(\"Current working directory:\", os.getcwd())\n\n# Import custom modules\nimport lib.func_file as ff\nfrom lib.visual_figure4 import performance_sigma_point\nfrom lib.cluster_visualizations import (\n    plot_silhouette_scores,\n    plot_cluster_distributions, \n    plot_shape_functions_by_cluster, \n    plot_cluster_2d_projections,\n    plot_cluster_size_distribution,\n    plot_cluster_centers,\n    plot_model_performance_comparison,\n    plot_sample_predictions,\n    plot_error_distributions,\n    plot_overall_performance_comparison\n)\n\n# Set up directories\ncwd_data = cwd + '/Data/'\ncwd_output = cwd + '/output/'\nos.makedirs(cwd_output, exist_ok=True)\n\n# Set device for PyTorch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import importlib\n",
    "\n",
    "# Check and install required packages\n",
    "if importlib.util.find_spec(\"torch\") is None:\n",
    "    !pip install torch\n",
    "if importlib.util.find_spec(\"zarr\") is None:\n",
    "    !pip install zarr\n",
    "if importlib.util.find_spec(\"scikit-learn\") is None:\n",
    "    !pip install scikit-learn\n",
    "\n",
    "# Ensure xarray is upgraded\n",
    "!pip install --upgrade xarray"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# This cell is no longer needed since the imports are already in cell 1\n# Keeping this as a comment for reference"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load and Preprocess Data\n",
    "\n",
    "We'll load the GOTM (General Ocean Turbulence Model) data provided by Sane et al. (2023) and apply the same preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load GOTM training data produced by Sane et al. 2023\nstore = 'https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/sf_training_data.zarr'\nd = xr.open_dataset(store, engine='zarr', chunks={})\n\n# Coriolis parameter calculation\ndef corio(lat):\n    return 2*(2*np.pi/(24*60*60)) * np.sin(lat*(np.pi/180))\n\n# Extract variables\nl0 = corio(d['l'][:])\nb00 = d['b0'][:]\nustar0 = d['ustar'][:]\nh0 = d['h'][:]\nlat0 = d['lat'][:]\nheat0 = d['heat'][:]\ntx0 = d['tx'][:]\ntx0 = np.round(tx0, 2)\nSF0 = d['SF'][:]\n\n# Print summary of input shape\nprint(f\"Data shape: {SF0.shape}, representing {SF0.shape[0]} samples with {SF0.shape[1]} vertical levels\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of input features\n",
    "variables = {\n",
    "    \"Coriolis parameter (l0) (s$^{-1}$)\": l0.values.flatten(),\n",
    "    \"Surface Buoyancy Flux (b00) (m$^{2}$ s$^{-3}$)\": b00.values.flatten(),\n",
    "    \"Surface Friction Velocity (ustar0) (m s$^{-1}$)\": ustar0.values.flatten(),\n",
    "    \"Boundary Layer Depth (h0) (m)\": h0.values.flatten()\n",
    "}\n",
    "\n",
    "# Create histograms with improved styling\n",
    "plt.figure(figsize=(15, 10))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for i, (name, data) in enumerate(variables.items()):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.hist(data, bins=30, color=colors[i], alpha=0.7, edgecolor='black')\n",
    "    plt.title(name, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Value\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of Input Features\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Apply filtering criteria as in Sane et al. (2023)\nprint(\"Original number of samples:\", len(heat0))\n\n# Create masks for each filtering criterion\nmask_heat = np.abs(heat0) < 601  # Heat flux limit\nmask_wind = tx0 < 1.2            # Wind stress limit\nmask_depth_min = h0 > 29         # Minimum layer depth\nmask_depth_max = h0 < 301        # Maximum layer depth\n\n# Combine all masks using logical AND\ncombined_mask = mask_heat & mask_wind & mask_depth_min & mask_depth_max\n\n# Get filtered indices\nfiltered_indices = np.where(combined_mask)[0]\n\nprint(\"Filtering statistics:\")\nprint(f\"Heat flux filter: {np.sum(mask_heat)} samples passed ({np.sum(mask_heat)/len(heat0)*100:.1f}%)\")\nprint(f\"Wind stress filter: {np.sum(mask_wind)} samples passed ({np.sum(mask_wind)/len(heat0)*100:.1f}%)\")\nprint(f\"Min depth filter: {np.sum(mask_depth_min)} samples passed ({np.sum(mask_depth_min)/len(heat0)*100:.1f}%)\")\nprint(f\"Max depth filter: {np.sum(mask_depth_max)} samples passed ({np.sum(mask_depth_max)/len(heat0)*100:.1f}%)\")\nprint(f\"Combined: {len(filtered_indices)} samples passed ({len(filtered_indices)/len(heat0)*100:.1f}%)\")\n\n# Prepare data for model training using the combined mask\nmm1 = 0; mm2 = 16  # 16 levels (level 1 at bottom, level 16 at top)\ndata_load_main = np.zeros([len(filtered_indices), 4 + mm2 - mm1])\ndata_load_main[:, 0] = l0[filtered_indices]     # Coriolis parameter\ndata_load_main[:, 1] = b00[filtered_indices]    # Surface buoyancy flux\ndata_load_main[:, 2] = ustar0[filtered_indices] # Surface friction velocity\ndata_load_main[:, 3] = h0[filtered_indices]     # Boundary layer depth\ndata_load_main[:, 4:(mm2 - mm1 + 4)] = SF0[filtered_indices, mm1:mm2] # Shape functions\n\n# Store additional forcing variables for reference\ndata_forc = np.zeros([len(filtered_indices), 3])\ndata_forc[:, 0] = lat0[filtered_indices]  # Latitude\ndata_forc[:, 1] = heat0[filtered_indices] # Heat flux\ndata_forc[:, 2] = tx0[filtered_indices]   # Wind stress\n\n# Create a copy of the data for processing\ndata_load3 = copy.deepcopy(data_load_main)\n\n# Preprocess data using the function from the original code\nprint('Preprocessing data...')\ndata, x, y, stats, k_mean, k_std = ff.preprocess_train_data(data_load3)\nprint('Done preprocessing')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "url = \"https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/data_testing_4_paper.txt\"\n",
    "df = pd.read_csv(url, delim_whitespace=True, header=None)\n",
    "valid_data = df.iloc[:, 3:].values\n",
    "\n",
    "# Apply same filtering criteria\n",
    "ind3 = np.where(valid_data[:, 3] > 29)[0]\n",
    "ind4 = np.where(valid_data[:, 3] < 301)[0]\n",
    "ind = np.intersect1d(ind3, ind4)\n",
    "\n",
    "# Extract and normalize validation features\n",
    "valid_x = valid_data[ind, 0:4]\n",
    "valid_x[:, 0] = (valid_x[:, 0] - stats[0]) / stats[1]  # Normalize Coriolis\n",
    "valid_x[:, 1] = (valid_x[:, 1] - stats[2]) / stats[3]  # Normalize buoyancy flux\n",
    "valid_x[:, 2] = (valid_x[:, 2] - stats[4]) / stats[5]  # Normalize friction velocity\n",
    "valid_x[:, 3] = (valid_x[:, 3] - stats[6]) / stats[7]  # Normalize layer depth\n",
    "\n",
    "# Extract and normalize validation targets\n",
    "valid_y = valid_data[ind, 5:]\n",
    "for i in range(len(valid_y)):\n",
    "    valid_y[i, :] = np.log(valid_y[i, :] / np.max(valid_y[i, :]))\n",
    "for i in range(16):\n",
    "    valid_y[:, i] = (valid_y[:, i] - k_mean[i]) / k_std[i]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.FloatTensor(x).to(device)\n",
    "y = torch.FloatTensor(y).to(device)\n",
    "valid_x = torch.FloatTensor(valid_x).to(device)\n",
    "valid_y = torch.FloatTensor(valid_y).to(device)\n",
    "\n",
    "print(f\"Training data shape: {x.shape}, {y.shape}\")\n",
    "print(f\"Validation data shape: {valid_x.shape}, {valid_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define the Neural Network Model\n",
    "\n",
    "We'll use the same architecture as in the original paper: a multilayer perceptron with 2 hidden layers and 32 neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OceanMixingNN(nn.Module):\n",
    "    def __init__(self, in_nodes, hidden_nodes, out_nodes):\n",
    "        super(OceanMixingNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_nodes, hidden_nodes)\n",
    "        self.linear2 = nn.Linear(hidden_nodes, hidden_nodes)\n",
    "        self.linear3 = nn.Linear(hidden_nodes, out_nodes)\n",
    "        self.dropout = nn.Dropout(0.25)  # Regularization\n",
    "        \n",
    "        # Initialize weights using the same approach as in original paper\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.xavier_uniform_(self.linear3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def train_model(model, x, y, valid_x, valid_y, k_mean, k_std, epochs=3000, lr=1e-3, patience=50):\n    \"\"\"Train a model with early stopping based on validation loss\"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.L1Loss(reduction='mean')\n    losses = []\n    \n    # Convert k_mean and k_std to tensors on appropriate device\n    k_mean_tensor = torch.tensor(k_mean, dtype=torch.float32).to(device)\n    k_std_tensor = torch.tensor(k_std, dtype=torch.float32).to(device)\n    \n    # For early stopping\n    best_loss = float('inf')\n    best_model = None\n    no_improve = 0\n    \n    with tqdm(total=epochs, desc=\"Training\") as pbar:\n        for epoch in range(epochs):\n            # Training step\n            model.train()\n            optimizer.zero_grad()\n            \n            # Forward pass\n            train_pred = model(x)\n            \n            # Compute loss (same as in original paper)\n            loss = loss_fn(train_pred, y)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            # Validation\n            model.eval()\n            with torch.no_grad():\n                valid_pred = model(valid_x)\n                \n                # Calculate loss in original space, not normalized space\n                train_loss = torch.mean(torch.abs(\n                    torch.exp(train_pred * k_std_tensor + k_mean_tensor) - \n                    torch.exp(y * k_std_tensor + k_mean_tensor)\n                ))\n                \n                valid_loss = torch.mean(torch.abs(\n                    torch.exp(valid_pred * k_std_tensor + k_mean_tensor) - \n                    torch.exp(valid_y * k_std_tensor + k_mean_tensor)\n                ))\n            \n            # Store losses\n            losses.append((epoch, train_loss.item(), valid_loss.item()))\n            \n            # Update progress bar\n            pbar.update(1)\n            pbar.set_postfix({\n                'train_loss': f\"{train_loss.item():.4f}\", \n                'valid_loss': f\"{valid_loss.item():.4f}\",\n                'patience': no_improve\n            })\n            \n            # Early stopping\n            if valid_loss.item() < best_loss:\n                best_loss = valid_loss.item()\n                best_model = copy.deepcopy(model.state_dict())\n                no_improve = 0\n            else:\n                no_improve += 1\n                if no_improve >= patience:\n                    print(f\"\\nEarly stopping at epoch {epoch+1}\")\n                    break\n    \n    # Load the best model\n    if best_model is not None:\n        model.load_state_dict(best_model)\n    \n    return model, np.array(losses)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train the Baseline Model\n",
    "\n",
    "First, we'll train a single model on all data, following the exact approach in Sane et al. (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "in_nodes = 4      # 4 input features\n",
    "hidden_nodes = 32 # 32 nodes per hidden layer\n",
    "out_nodes = 16    # 16 output points (one for each sigma level)\n",
    "\n",
    "# Initialize and train the baseline model\n",
    "print(\"Training baseline model on all data...\")\n",
    "baseline_model = OceanMixingNN(in_nodes, hidden_nodes, out_nodes).to(device)\n",
    "baseline_model, baseline_losses = train_model(\n",
    "    baseline_model, x, y, valid_x, valid_y, k_mean, k_std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = baseline_losses[:, 0]\n",
    "train_loss = baseline_losses[:, 1]\n",
    "valid_loss = baseline_losses[:, 2]\n",
    "\n",
    "plt.plot(epochs, train_loss, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(epochs, valid_loss, 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.fill_between(epochs, train_loss, valid_loss, alpha=0.2, color='gray', label='Generalization Gap')\n",
    "\n",
    "# Add the best validation loss\n",
    "best_epoch = np.argmin(valid_loss)\n",
    "best_valid = valid_loss[best_epoch]\n",
    "plt.scatter(epochs[best_epoch], best_valid, c='g', s=100, zorder=3)\n",
    "plt.annotate(f'Best: {best_valid:.4f}', \n",
    "             (epochs[best_epoch], best_valid),\n",
    "             xytext=(10, -20), textcoords='offset points',\n",
    "             arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('L1 Loss', fontsize=14)\n",
    "plt.title('Baseline Model Training', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering Analysis\n",
    "\n",
    "Now we'll implement our extension to the original approach by clustering the ocean states and training separate models for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Feature Clustering\n",
    "\n",
    "We'll cluster the input data based on the four physical parameters: Coriolis parameter, surface buoyancy flux, surface friction velocity, and boundary layer depth."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def find_optimal_clusters(features, max_clusters=10, fixed_clusters=None):\n    \"\"\"\n    Find optimal number of clusters using silhouette score.\n    If fixed_clusters is provided, will return that value without calculation.\n    \"\"\"\n    # If fixed number of clusters is provided, return it\n    if fixed_clusters is not None:\n        print(f\"Using fixed number of clusters: {fixed_clusters}\")\n        return fixed_clusters\n    \n    # Scale features for clustering\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n    \n    # Calculate silhouette scores for different cluster counts\n    silhouette_scores_list = []\n    for k in range(2, max_clusters + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        cluster_labels = kmeans.fit_predict(scaled_features)\n        score = silhouette_score(scaled_features, cluster_labels)\n        silhouette_scores_list.append((k, score))\n        print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")\n    \n    # Plot silhouette scores using our visualization function\n    plot_silhouette_scores(silhouette_scores_list)\n    \n    # Return optimal number of clusters (highest silhouette score)\n    return max(silhouette_scores_list, key=lambda x: x[1])[0]"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Get original (non-normalized) input features for clustering\noriginal_features = data_load_main[:, :4].astype(np.float64)\n\n# Use a fixed number of clusters (4) to start with\nn_clusters = 4\nprint(f\"Using a fixed number of clusters: {n_clusters}\")\n\n# Later, if you want to find the optimal number of clusters, you can use:\n# n_clusters = find_optimal_clusters(original_features, max_clusters=8)\n# But be aware this can be time-consuming for large datasets"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def cluster_data(features, n_clusters):\n    \"\"\"Cluster the data based on physical parameters\"\"\"\n    # Scale features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    cluster_assignments = kmeans.fit_predict(scaled_features)\n    \n    # Print cluster sizes\n    for i in range(n_clusters):\n        count = np.sum(cluster_assignments == i)\n        print(f\"Cluster {i}: {count} samples ({count/len(cluster_assignments)*100:.2f}%)\")\n    \n    # Extract cluster centroids in original feature space\n    centroids_scaled = kmeans.cluster_centers_\n    centroids_original = scaler.inverse_transform(centroids_scaled)\n    \n    # Create a DataFrame with cluster statistics\n    feature_names = ['Coriolis', 'Buoyancy Flux', 'Friction Velocity', 'Layer Depth']\n    cluster_stats = pd.DataFrame(centroids_original, columns=feature_names)\n    cluster_stats.index = [f\"Cluster {i}\" for i in range(n_clusters)]\n    \n    # Add standard deviations\n    for i in range(n_clusters):\n        mask = cluster_assignments == i\n        cluster_features = features[mask]\n        stds = np.std(cluster_features, axis=0)\n        for j, col in enumerate(cluster_stats.columns):\n            cluster_stats.at[f\"Cluster {i}\", f\"{col} StdDev\"] = stds[j]\n    \n    # Visualize cluster distributions\n    print(\"\\nVisualizaing cluster feature distributions...\")\n    plot_cluster_distributions(features, cluster_assignments, n_clusters, feature_names)\n    \n    # Visualize cluster centers\n    print(\"\\nVisualizaing cluster centers...\")\n    plot_cluster_centers(kmeans, feature_names, scaler)\n    \n    # Visualize cluster size distribution\n    print(\"\\nVisualizaing cluster size distribution...\")\n    plot_cluster_size_distribution(cluster_assignments, n_clusters)\n    \n    return cluster_assignments, scaler, kmeans, cluster_stats"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def visualize_shape_functions(data_load_main, cluster_assignments, n_clusters):\n    \"\"\"\n    Visualize the shape functions for each cluster and 2D projections of clusters.\n    \n    Parameters:\n        data_load_main: Raw data containing shape functions\n        cluster_assignments: Cluster labels\n        n_clusters: Number of clusters\n    \"\"\"\n    # Extract input features\n    features = data_load_main[:, :4]\n    feature_names = ['Coriolis Parameter', 'Surface Buoyancy Flux', \n                     'Surface Friction Velocity', 'Boundary Layer Depth']\n    \n    # Visualize shape functions by cluster\n    print(\"Visualizing mean shape functions by cluster...\")\n    shape_function_fig = plot_shape_functions_by_cluster(data_load_main, cluster_assignments, n_clusters)\n    \n    # Visualize 2D projections of features colored by cluster\n    print(\"Visualizing 2D projections of clusters...\")\n    projection_fig = plot_cluster_2d_projections(features, cluster_assignments, n_clusters, feature_names)\n    \n    return shape_function_fig, projection_fig"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Cluster the data\nprint(\"Clustering the data with\", n_clusters, \"clusters...\")\ncluster_assignments, scaler, kmeans, cluster_stats = cluster_data(original_features, n_clusters)\n\n# Display cluster statistics for better understanding\nprint(\"\\nCluster Centers and Statistics:\")\ndisplay(cluster_stats)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the clusters\nshape_function_fig, projection_fig = visualize_shape_functions(data_load_main, cluster_assignments, n_clusters)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train Cluster-Specific Models\n",
    "\n",
    "Now we'll train separate models for each cluster and compare their performance with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def train_cluster_models(x, y, valid_x, valid_y, k_mean, k_std, cluster_assignments, n_clusters):\n    \"\"\"Train separate models for each cluster\"\"\"\n    # Convert variables to numpy for indexing\n    x_np = x.cpu().numpy()\n    y_np = y.cpu().numpy()\n    \n    # Initialize a dictionary to store models\n    cluster_models = {}\n    cluster_losses = {}\n    \n    # Train a model for each cluster\n    for cluster_id in range(n_clusters):\n        print(f\"\\nTraining model for Cluster {cluster_id}\")\n        \n        # Clear CUDA cache to avoid memory issues\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        # Get data points belonging to this cluster\n        cluster_mask = cluster_assignments == cluster_id\n        x_cluster = torch.FloatTensor(x_np[cluster_mask]).to(device)\n        y_cluster = torch.FloatTensor(y_np[cluster_mask]).to(device)\n        \n        print(f\"Cluster {cluster_id} data shape: {x_cluster.shape}, {y_cluster.shape}\")\n        \n        # Initialize and train model\n        model = OceanMixingNN(in_nodes, hidden_nodes, out_nodes).to(device)\n        \n        # Adjust patience based on cluster size to avoid early stopping\n        cluster_size = x_cluster.shape[0]\n        adjusted_patience = max(20, min(50, int(cluster_size / 100)))\n        print(f\"Using patience of {adjusted_patience} epochs for early stopping\")\n        \n        model, losses = train_model(\n            model, x_cluster, y_cluster, valid_x, valid_y, k_mean, k_std,\n            patience=adjusted_patience\n        )\n        \n        # Store model and losses\n        cluster_models[cluster_id] = model\n        cluster_losses[cluster_id] = losses\n    \n    return cluster_models, cluster_losses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train cluster-specific models\n",
    "cluster_models, cluster_losses = train_cluster_models(\n",
    "    x, y, valid_x, valid_y, k_mean, k_std, cluster_assignments, n_clusters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# This function is simple enough to keep as is\ndef plot_cluster_losses(cluster_losses, n_clusters):\n    \"\"\"Plot training and validation losses for each cluster model\"\"\"\n    # Set up colors\n    colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n    \n    plt.figure(figsize=(12, 8))\n    \n    for i in range(n_clusters):\n        losses = cluster_losses[i]\n        epochs = losses[:, 0]\n        valid_loss = losses[:, 2]\n        \n        plt.plot(epochs, valid_loss, '-', color=colors[i], linewidth=2, \n                label=f'Cluster {i} Validation Loss')\n        \n        # Mark the best validation loss\n        best_epoch = np.argmin(valid_loss)\n        best_valid = valid_loss[best_epoch]\n        plt.scatter(epochs[best_epoch], best_valid, c=colors[i], s=80, zorder=3)\n        \n    # Add baseline validation loss for comparison\n    baseline_epochs = baseline_losses[:, 0]\n    baseline_valid = baseline_losses[:, 2]\n    plt.plot(baseline_epochs, baseline_valid, 'k-', linewidth=3, \n            label='Baseline Validation Loss')\n    \n    best_baseline_epoch = np.argmin(baseline_valid)\n    best_baseline = baseline_valid[best_baseline_epoch]\n    plt.scatter(baseline_epochs[best_baseline_epoch], best_baseline, c='k', s=100, zorder=3)\n    plt.annotate(f'Baseline: {best_baseline:.4f}', \n                 (baseline_epochs[best_baseline_epoch], best_baseline),\n                 xytext=(10, 10), textcoords='offset points',\n                 arrowprops=dict(arrowstyle='->', color='black'))\n    \n    plt.grid(True, alpha=0.3, linestyle='--')\n    plt.xlabel('Epochs', fontsize=14)\n    plt.ylabel('Validation Loss', fontsize=14)\n    plt.title('Validation Losses by Cluster', fontsize=16, fontweight='bold')\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n    \n    return plt.gcf()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses for each cluster model\n",
    "plot_cluster_losses(cluster_losses, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Performance Evaluation\n",
    "\n",
    "We'll now evaluate the performance of our cluster-specific models compared to the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Make predictions using both models\nprint(\"Generating predictions with baseline model...\")\nwith torch.no_grad():\n    baseline_preds = baseline_model(valid_x)\n    \n# Make sure predict_with_cluster_models is defined before using it\ndef predict_with_cluster_models(x_input, cluster_models, scaler, kmeans):\n    \"\"\"Make predictions using appropriate cluster-specific models\"\"\"\n    # Determine which cluster each validation sample belongs to\n    if isinstance(x_input, torch.Tensor):\n        x_np = x_input.cpu().numpy()\n    else:\n        x_np = x_input\n    \n    # Original features need to be unnormalized for clustering\n    unnormalized_x = x_np.copy()\n    unnormalized_x[:, 0] = unnormalized_x[:, 0] * stats[1] + stats[0]  # Coriolis\n    unnormalized_x[:, 1] = unnormalized_x[:, 1] * stats[3] + stats[2]  # Buoyancy\n    unnormalized_x[:, 2] = unnormalized_x[:, 2] * stats[5] + stats[4]  # Friction\n    unnormalized_x[:, 3] = unnormalized_x[:, 3] * stats[7] + stats[6]  # Depth\n    \n    # Scale features for clustering\n    scaled_features = scaler.transform(unnormalized_x)\n    \n    # Assign clusters\n    val_clusters = kmeans.predict(scaled_features)\n    \n    # Make predictions\n    all_preds = []\n    for i in range(len(x_input)):\n        cluster_id = val_clusters[i]\n        model = cluster_models[cluster_id]\n        \n        # Get input as tensor\n        if isinstance(x_input, torch.Tensor):\n            x_i = x_input[i:i+1]\n        else:\n            x_i = torch.FloatTensor(x_np[i:i+1]).to(device)\n        \n        # Predict\n        with torch.no_grad():\n            pred = model(x_i)\n            all_preds.append(pred)\n    \n    # Combine predictions\n    return torch.cat(all_preds, dim=0), val_clusters\n\nprint(\"Generating predictions with cluster-specific models...\")\ncluster_preds, val_clusters = predict_with_cluster_models(valid_x, cluster_models, scaler, kmeans)\n\n# Calculate performance metrics in original space\nk_mean_tensor = torch.tensor(k_mean, dtype=torch.float32).to(device)\nk_std_tensor = torch.tensor(k_std, dtype=torch.float32).to(device)\n\n# Transform predictions back to original space\nbaseline_preds_orig = torch.exp(baseline_preds * k_std_tensor + k_mean_tensor)\ncluster_preds_orig = torch.exp(cluster_preds * k_std_tensor + k_mean_tensor)\nvalid_y_orig = torch.exp(valid_y * k_std_tensor + k_mean_tensor)\n\n# Calculate node-wise losses for both models\nbaseline_node_losses = []\ncluster_node_losses = []\n\nfor i in range(valid_y_orig.shape[1]):\n    baseline_loss = torch.mean(torch.abs(baseline_preds_orig[:, i] - valid_y_orig[:, i])).item()\n    cluster_loss = torch.mean(torch.abs(cluster_preds_orig[:, i] - valid_y_orig[:, i])).item()\n    \n    baseline_node_losses.append(baseline_loss)\n    cluster_node_losses.append(cluster_loss)\n\nprint(\"Overall Baseline Loss:\", np.mean(baseline_node_losses))\nprint(\"Overall Cluster Model Loss:\", np.mean(cluster_node_losses))\nimprovement = (np.mean(baseline_node_losses) - np.mean(cluster_node_losses)) / np.mean(baseline_node_losses) * 100\nprint(f\"Improvement: {improvement:.2f}%\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def visualize_model_performance(baseline_preds_orig, cluster_preds_orig, valid_y_orig, \n                              baseline_node_losses, cluster_node_losses, val_clusters):\n    \"\"\"Visualize performance comparison between baseline and cluster models\"\"\"\n    # Convert tensors to numpy for visualization\n    if isinstance(baseline_preds_orig, torch.Tensor):\n        baseline_np = baseline_preds_orig.cpu().numpy()\n        cluster_np = cluster_preds_orig.cpu().numpy()\n        valid_np = valid_y_orig.cpu().numpy()\n    else:\n        baseline_np = baseline_preds_orig\n        cluster_np = cluster_preds_orig\n        valid_np = valid_y_orig\n    \n    # 1. Node-wise performance comparison\n    print(\"Visualizing node-wise performance improvements...\")\n    node_perf_fig = plot_model_performance_comparison(baseline_node_losses, cluster_node_losses)\n    \n    # 2. Overall performance comparison\n    print(\"Visualizing overall performance comparison...\")\n    baseline_loss = np.mean(baseline_node_losses)\n    cluster_loss = np.mean(cluster_node_losses)\n    overall_perf_fig = plot_overall_performance_comparison(baseline_loss, cluster_loss)\n    \n    # 3. Error distribution comparison\n    print(\"Visualizing error distributions...\")\n    error_fig = plot_error_distributions(baseline_np, cluster_np, valid_np)\n    \n    # 4. Sample predictions\n    print(\"Visualizing sample predictions...\")\n    # Select samples from different clusters\n    unique_clusters = np.unique(val_clusters)\n    sample_indices = []\n    for cluster_id in unique_clusters:\n        cluster_mask = val_clusters == cluster_id\n        if np.sum(cluster_mask) > 0:\n            samples = np.where(cluster_mask)[0]\n            sample_idx = np.random.choice(samples)\n            sample_indices.append(sample_idx)\n    \n    # Limit to 3 samples\n    sample_indices = sample_indices[:3]\n    sample_fig = plot_sample_predictions(sample_indices, val_clusters, \n                                       baseline_np, cluster_np, valid_np)\n    \n    return node_perf_fig, overall_perf_fig, error_fig, sample_fig"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization of performance comparison\nnode_perf_fig, overall_perf_fig, error_fig, sample_fig = visualize_model_performance(\n    baseline_preds_orig, cluster_preds_orig, valid_y_orig, \n    baseline_node_losses, cluster_node_losses, val_clusters\n)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Combining multiple visualizations for a comprehensive performance summary\nplt.figure(figsize=(15, 12))\ngs = GridSpec(2, 2)\n\n# Plot 1: Node-wise performance comparison\nax1 = plt.subplot(gs[0, 0])\nplt.sca(node_perf_fig.axes[0])\nplt.title('Performance Improvement by Layer', fontsize=14, fontweight='bold')\n\n# Plot 2: Overall performance comparison\nax2 = plt.subplot(gs[0, 1])\nplt.sca(overall_perf_fig.axes[0])\nplt.title('Overall Model Performance', fontsize=14, fontweight='bold')\n\n# Plot 3: Error distributions\nax3 = plt.subplot(gs[1, 0])\nplt.sca(error_fig.axes[0])\nplt.title('Error Distribution', fontsize=14, fontweight='bold')\n\n# Plot 4: Sample prediction comparison\nax4 = plt.subplot(gs[1, 1])\nif len(sample_fig.axes) > 0:\n    plt.sca(sample_fig.axes[0])\n    plt.title('Sample Prediction', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.suptitle('Cluster-Based vs. Baseline Model Performance', fontsize=18, fontweight='bold', y=1.02)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Discussion and Future Work\n",
    "\n",
    "## 3.1 Key Findings\n",
    "\n",
    "1. **Distinct Oceanic Regimes**: We identified distinct clusters in the ocean state space with different characteristic shape functions, confirming our hypothesis that ocean mixing processes vary systematically across different physical regimes.\n",
    "\n",
    "2. **Performance Improvement**: The cluster-specific models show improvement over the baseline model, demonstrating that conditional modeling can capture more nuanced relationships between physical parameters and vertical mixing profiles.\n",
    "\n",
    "3. **Layer-Specific Improvements**: The improvement varies across different vertical layers, suggesting that some parts of the water column benefit more from the cluster-based approach than others.\n",
    "\n",
    "## 3.2 Implications\n",
    "\n",
    "These findings suggest that ocean mixing parameterizations could benefit from a regime-based approach, where different models are applied depending on the oceanic conditions. This could lead to more accurate representations of vertical mixing in ocean models, ultimately improving climate simulations.\n",
    "\n",
    "## 3.3 Future Work\n",
    "\n",
    "1. **Optimal Clustering Approach**: Explore different clustering methods (e.g., Gaussian Mixture Models, hierarchical clustering) and compare their effectiveness.\n",
    "\n",
    "2. **Cluster Selection Mechanism**: Develop a smooth transition method between clusters to avoid discontinuities when ocean states evolve over time.\n",
    "\n",
    "3. **Output-Based Clustering**: Instead of clustering on input features, cluster on output shape functions to identify regimes based on their mixing behavior directly.\n",
    "\n",
    "4. **Physical Interpretation**: Analyze cluster characteristics more deeply to understand the physical mechanisms driving different mixing regimes.\n",
    "\n",
    "5. **Global Model Implementation**: Test the clustered approach in a full global ocean model to evaluate its impact on larger-scale ocean and climate dynamics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}