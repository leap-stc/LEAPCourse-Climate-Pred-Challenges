{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterizing Vertical Mixing Coefficients in the Ocean Surface Boundary Layer Using Neural Networks\n",
    "\n",
    "\n",
    "In this notebook, we reproduce a portion of the analysis carried out by  \n",
    "**Sane et al. (2023) \"Parameterizing vertical mixing coefficients in the ocean surface boundary layer using neural networks.\" Journal of Advances in Modeling Earth Systems 15.10: [e2023MS003890](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003890)** (with original code and data at https://zenodo.org/records/8293998)\n",
    "\n",
    "### Tasks Covered Here\n",
    "- **Data Processing and Preparation**: Reading and filtering output from the Generalized Ocean Turbulence Model (GOTM) (GOTM is section 3.3 of Sane et al. 2023).\n",
    "- **Neural Network Training**: Implementing neural networks to predict shape function $ g(\\sigma) $ based on input physical parameters ($ \\mathcal{N}_1 $.).\n",
    "- **Hyperparameter Optimization**: Conducting a hyperparameter sweep to evaluate the effect of different model architectures (e.g., number of layers, nodes) on performance.\n",
    "- **Model Evaluation and Visualization**: Analyzing the ML model skill using training/validation loss\n",
    "- **Visualizing results**: Reproducing Figures 1-4 of Sane et al. 2023.\n",
    "  \n",
    "### Estimated Compute Time\n",
    "- On LEAP Pangeo GPU configuration in February 2025, this notebook should run to completion in ~10 minutes\n",
    "- This notebook will take several hours on LEAP-Pangeo education CPU configuration (4 CPU, 32GB RAM). \n",
    "- Options for speed up include lowering patience to triggering faster early stopping or further limiting hyperparameter sweep. \n",
    "\n",
    "### Collaboration Guidelines for Climate Prediction Challenges with Machine Learning (Spring 2025), Columbia University Earth and Environmental Sciences and Statistics\n",
    "- GitHub sharing is recommended. \n",
    "  - We have prepared a [Github Tutorial](https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges/blob/main/Tutorials/Github-Tutorial.md) to help. \n",
    "  - If this is not working for you, please reach out with questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup workspace and Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import importlib\n",
    "\n",
    "# Check and install torch\n",
    "if importlib.util.find_spec(\"torch\") is None:\n",
    "    !pip install torch\n",
    "\n",
    "# Check and install zarr\n",
    "if importlib.util.find_spec(\"zarr\") is None:\n",
    "    !pip install zarr\n",
    "\n",
    "# Ensure xarray is upgraded\n",
    "!pip install --upgrade xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy as copy\n",
    "import matplotlib as mpl\n",
    "import netCDF4 as ncd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "import matplotlib.cm as cm\n",
    "import copy as copy\n",
    "import multiprocessing as mp\n",
    "from scipy import stats\n",
    "import time as time\n",
    "import matplotlib.font_manager\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "import xarray as xr\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "today = datetime.today()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cwd=os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "os.chdir(parent_dir)\n",
    "cwd = parent_dir\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# custom modules\n",
    "import lib.func_file as ff\n",
    "from lib.visual_figure4 import performance_sigma_point\n",
    "from lib.visual_figure3 import score_eval, save_losses_by_seed, plot_n1_scores\n",
    "from lib.improved_visualizations import (\n",
    "    plot_input_distributions, \n",
    "    plot_diffusivity_distributions, \n",
    "    plot_correlation_heatmap,\n",
    "    plot_shape_functions,\n",
    "    plot_training_validation_loss,\n",
    "    plot_enhanced_performance,\n",
    "    plot_hyperparameter_comparison,\n",
    "    plot_cluster_analysis\n",
    ")\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "cwd_data = cwd + '/Data/'\n",
    "cwd_output = cwd + '/output/'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertical mixing in the ocean surface boundary layer (OSBL) plays a vital role in regulating the exchange of heat, momentum, and carbon between the atmosphere and the ocean interior. As a key determinant in the rate of ocean heat uptake, the OBSL plays a central role in the ocean's modulation of global air temperatures and in the sea level rise that results from a warming ocean. The OBSL also modulates the rate the ocean absorbs anthropogenic CO$_2$, thus playing a key role in the ocean's modulation atmospheric CO$_2$ levels, and thus, on climate change. If these processes are to be confidently projected for the future, it is critical that the OBSL be accurately represented in ocean models and the ocean components of Earth System Models.  \n",
    "\n",
    "In sum, accurate parameterization of upper ocean vertical mixing is a critical component of ocean and climate modeling.  \n",
    "\n",
    "Upper ocean vertical mixing typically represented as a vertically-varying diffusivity parameter $ \\kappa_\\phi (\\sigma) $, where $\\kappa$ is a variable diffisivity of scalar $\\phi$, and $\\sigma$ is a vertical index scaled to the depth of the upper ocean mixed layer.\n",
    "\n",
    "Traditionally, vertical diffusivity is parameterized using **universal shape functions**, such as the dashed line shown in the first figure below. These fixed, predefined profiles assume uniformity across diverse ocean conditions. However, as illustrated in Figure 1, shape functions derived from more sophisticated, but more computationally expensive, second moment closure (SMC) methods (blue shaded region) demonstrate significant variability under different forcing conditions. The discrepancy between the universal shape function and SMC-derived profiles highlights a fundamental limitation: universal functions fail to capture the dynamic, environment-dependent behavior of vertical mixing in the OBSL.\n",
    "\n",
    "This deficiency motivates the need for a more adaptive and accurate approach. Sane et al. (2023) propose a **neural network-based model** to replace the universal shape function. Neural networks offer the flexibility to learn complex, nonlinear relationships from data and can dynamically adjust the shape function to reflect varying physical conditions. This data-driven, physics-aware method bridges the gap between traditional parameterization schemes and the variability observed in high-fidelity SMC models. Though not the focus here, the approach of Sane et al. (2023) is also designed for seamless integration and computational efficiency in existing global ocean model codes; the impacts of this implementation is illustrated in section 4 of the paper. \n",
    "\n",
    "The approach integrates two neural networks:\n",
    "\n",
    "1. $ \\mathcal{N}_1 $: Predicts the shape function $ g(\\sigma) $, capturing the vertical structure of diffusivity.\n",
    "2. $ \\mathcal{N}_2 $: Predicts the velocity scale $ v_0 $, which, combined with $ g(\\sigma) $, yields the full diffusivity profile.\n",
    "\n",
    "By leveraging neural networks, the authors aim to enhance the accuracy and generalizability of vertical mixing parameterizations in OGCMs, enabling more reliable ocean and climate predictions.\n",
    "\n",
    "Here, we will focus on predicting the shape function $ g(\\sigma) $ for vertical diffusivity using the neural network  $ \\mathcal{N}_1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GOTM training data produced by Sane et al. 2023\n",
    "# Plot the assumed univeral shape function vs. GOTM results to illustrate the qualitative differences\n",
    "\n",
    "store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/raw_training_data.zarr'\n",
    "d = xr.open_dataset(store, engine='zarr', chunks={})\n",
    "\n",
    "h=d['h'][:]\n",
    "h_ind1=np.where(h>10)[0]\n",
    "h_ind2=np.where(h<700)[0]\n",
    "h_ind=np.intersect1d(h_ind1,h_ind2)\n",
    "sf1=d['SF'][h_ind,:]\n",
    "sf1=sf1[237945:237945+5000,:] # randomly selected few profiles.\n",
    "\n",
    "sf=np.zeros([len(sf1),16])\n",
    "\n",
    "sf1 = sf1.values if hasattr(sf1, \"values\") else sf1\n",
    "sf = sf1 / np.max(sf1, axis=1, keepdims=True)\n",
    "\n",
    "# define vertical coordinate\n",
    "sig=np.linspace(0,1,18)\n",
    "\n",
    "# define universal shape function, section 2\n",
    "# gamma = 2\n",
    "z=np.linspace(0,1,100)\n",
    "z1=z*(1-z)**2\n",
    "z1=z1/np.max(z1)\n",
    "\n",
    "sfmin=np.zeros(18)\n",
    "sfmax=np.zeros(18)\n",
    "sfmin[1:17]=np.min(sf,axis=0)\n",
    "sfmax[1:17]=np.max(sf,axis=0)\n",
    "\n",
    "# Create enhanced shape function plot\n",
    "plot_shape_functions(sig, sfmin, sfmax, z, z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, profiles are plotted upside down \n",
    "- $\\sigma$=1 is BOTTOM of the column\n",
    "- $\\sigma$=0 is TOP of the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Description\n",
    "\n",
    "### Overview of Data Generation\n",
    "The training data for $ \\mathcal{N}_1 $ is derived from outputs of the **General Ocean Turbulence Model (GOTM)** (Umlauf & Burchard, 2005; Umlauf et al. 2014). GOTM is a one-dimensional water column model designed for simulating vertical turbulent mixing processes in the ocean. It provides a flexible and modular framework for testing turbulence parameterizations and studying oceanic boundary layer dynamics. GOTM is widely used in oceanography and climate research to investigate turbulence fluxes and vertical diffusivity.\n",
    "\n",
    "For more information, documentation, and source code, visit the official GOTM website: [GOTM Homepage](https://gotm.net)\n",
    "\n",
    "GOTM calculates upper ocean turbulence directly and from these vertical diffusivity outputs, the shape function $g(\\sigma)$ we require can be derived. \n",
    "\n",
    "---\n",
    "\n",
    "In this project, we use the pre-processed GOTM dataset from([zenodo link for Sane et al. 2023](https://zenodo.org/records/8293998)). This dataset contains all necessary variables for training $ \\mathcal{N}_1 $. We do not run GOTM here. \n",
    "\n",
    "The dataset includes the following variables:\n",
    "\n",
    "- **l0 (Coriolis parameter)**: Coriolis parameter, calculated based on latitude. Typically called $f$. Units of 1/s.\n",
    "- **b0 (Surface Buoyancy Flux)**: Represents buoyancy flux at the surface. Units of m$^2$/s$^3$\n",
    "- **ustar0 (Surface Friction Velocity)**: Surface friction velocity, related to wind shear. Units of m/s.\n",
    "- **h0 (Boundary Layer Depth)**: The depth of the boundary layer in the ocean or atmosphere. Units of m.\n",
    "- **lat0 (Latitude)**: Geographical latitude in degrees. \n",
    "- **heat0 (Surface Heat Flux)**: Heat flux at the surface, measured in W/m².\n",
    "- **tx0 (Wind Stress)**: Surface wind stress, measured in N/m².\n",
    "- **Shape Function ($ \\text{SF} $)**: The normalized diffusivity profile ($ g(\\sigma) $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset from Sane et al. 2023, here stored on the cloud, accessible on LEAP-Pangeo\n",
    "store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/sf_training_data.zarr'\n",
    "d = xr.open_dataset(store, engine='zarr', chunks={})\n",
    "\n",
    "# this function calculates the coriolis parameter from input latitude \n",
    "# 2*Omega*sin(lat), where Omega is the Earth's rotation in 1/s\n",
    "def corio(lat):\n",
    "    return  2*(2*np.pi/(24*60*60)) * np.sin(lat*(np.pi/180))\n",
    "\n",
    "l0=corio(d['l'][:])\n",
    "b00=d['b0'][:]\n",
    "ustar0=d['ustar'][:]\n",
    "h0=d['h'][:]\n",
    "lat0=d['lat'][:]\n",
    "heat0=d['heat'][:]\n",
    "tx0=d['tx'][:] \n",
    "tx0=np.round(tx0,2)\n",
    "SF0=d['SF'][:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualization\n",
    "\n",
    "It is important to understand your data. Here we make plots of the input and output data to understand their distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    \"Coriolis parameter (l0) (s$^{-1}$)\": l0.values.flatten(),\n",
    "    \"Surface Buoyancy Flux (b00) (m$^{2}$ s$^{-3}$)\": b00.values.flatten(),\n",
    "    \"Surface Friction Velocity (ustar0) (m s$^{-1}$)\": ustar0.values.flatten(),\n",
    "    \"Boundary Layer Depth (h0) (m)\": h0.values.flatten(),\n",
    "    \"Latitude (lat0) (deg)\": lat0.values.flatten(),\n",
    "    \"Surface Heat Flux (heat0) (W m$^{-2}$)\": heat0.values.flatten(),\n",
    "    \"Wind Stress (tx0) (N m$^{-2}$)\": tx0.values.flatten(),\n",
    "}\n",
    "\n",
    "# Create enhanced input distributions plot\n",
    "plot_input_distributions(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced diffusivity distributions plot\n",
    "plot_diffusivity_distributions(SF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for correlation analysis\n",
    "data = {\n",
    "    \"l0\": l0.values.flatten(),\n",
    "    \"b00\": b00.values.flatten(),\n",
    "    \"ustar0\": ustar0.values.flatten(),\n",
    "    \"h0\": h0.values.flatten(),\n",
    "    \"lat0\": lat0.values.flatten(),\n",
    "    \"heat0\": heat0.values.flatten(),\n",
    "    \"tx0\": tx0.values.flatten(),\n",
    "}\n",
    "\n",
    "for i in range(16):\n",
    "    data[f\"SF0_{i+1}\"] = SF0[:, 16-i-1].values.flatten()\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create enhanced correlation heatmap\n",
    "plot_correlation_heatmap(df, np.zeros(16), np.zeros(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations with Input variables\n",
    "\n",
    "Recall \n",
    "- SF0_1 is adjacent to BOTTOM of OBSL\n",
    "- SF0_16 is adjacent to TOP of OBSL\n",
    "- $\\sigma = \\frac{z}{h}$ is a normalized vertical coordinate, so the $\\it{physical}$ depth of the OBSL is variable across the ocean (from a few meters in the lower latitudes and in summer at mid-latitudes, to 100s m in the mid-latitude winter, to >1000 m at high latitudes in winter)\n",
    "\n",
    "The correlation between SF0 at each level and the input variables varies across depths. For example, buoyancy flux (b00) is more correlated with diffusivity higher in the column but measures of wind (ustar0, tx0) are more correlated with diffuivity nearer to the bottom of the OBSL. \n",
    "\n",
    "Keep in mind the variation in magnitudes of diffusivity from the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Filtering\n",
    "To ensure high-quality and valid data for training, specific filtering criteria are applied by Sane et al. (2023):\n",
    "\n",
    "1. Heat flux ($ \\text{heat} $) magnitude below 601 $ W/m^2 $.\n",
    "2. Wind stress ($ \\text{tx} $) below 1.2 $ N/m^2 $.\n",
    "3. Boundary layer depth ($ \\text{h} $) between 30 m and 300 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the constraints defined above\n",
    "ind101=np.where(np.abs(heat0)<601)[0]\n",
    "ind1=ind101 \n",
    "ind2=np.where(tx0<1.2)[0]\n",
    "ind3=np.where(h0>29)[0]\n",
    "ind4=np.where(h0<301)[0]\n",
    "# Filter the data to only those that obey the constraints\n",
    "# ind7 is the final filter to indentify the data that will be input to N1 training\n",
    "ind5=np.intersect1d(ind1,ind2)\n",
    "ind6=np.intersect1d(ind3,ind5)\n",
    "ind7=np.intersect1d(ind4,ind6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 16 levels (1 at bottom, 16 at top)\n",
    "mm1=0; mm2=16  #0; 16\n",
    "# apply ind7 to keep only the profiles that meet the filtering criteria\n",
    "# Training uses only 4 inputs: Coriolis parameter (l0), buoyancy flux (b00), surface friction velocity (ustar0) and boundary layer depth (h0)\n",
    "data_load_main=np.zeros([len(h0[ind7]),4+mm2-mm1])\n",
    "data_load_main[:,0]=l0[ind7]\n",
    "data_load_main[:,1]=b00[ind7]\n",
    "data_load_main[:,2]=ustar0[ind7]\n",
    "data_load_main[:,3]=h0[ind7]\n",
    "data_load_main[:,4:(mm2-mm1+4)]=SF0[ind7,mm1:mm2]\n",
    "\n",
    "data_forc=np.zeros([len(ind7),3])\n",
    "data_forc[:,0]=lat0[ind7]\n",
    "data_forc[:,1]=heat0[ind7]\n",
    "data_forc[:,2]=tx0[ind7]\n",
    "\n",
    "data_load3=copy.deepcopy(data_load_main)\n",
    "\n",
    "print('started')\n",
    "\n",
    "data, x,y, stats, k_mean, k_std=ff.preprocess_train_data(data_load3)  \n",
    "# Note: ff.preprocess_train_data is a custom function defined in lib/func_file.py that normalizes, log-transforms and shuffles the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Validation Dataset\n",
    "\n",
    "In this project, training and validation datasets are generated independently through separate simulations. This practice ensures **strict independence** between the two datasets, avoiding issues like **data leakage** or overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_data=np.loadtxt(cwd_data+'data_testing_4_paper.txt')[:,3:]\n",
    "\n",
    "url = \"https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/data_testing_4_paper.txt\"\n",
    "df = pd.read_csv(url, delim_whitespace=True, header = None)  \n",
    "valid_data = df.iloc[:, 3:].values \n",
    "\n",
    "ind3=np.where(valid_data[:,3]>29)[0]\n",
    "ind4=np.where(valid_data[:,3]<301)[0]\n",
    "ind=np.intersect1d(ind3,ind4)\n",
    "\n",
    "valid_x=valid_data[ind,0:4]\n",
    "\n",
    "valid_x[:,0]=(valid_x[:,0]-stats[0])/stats[1]\n",
    "valid_x[:,1]=(valid_x[:,1]-stats[2])/stats[3]\n",
    "valid_x[:,2]=(valid_x[:,2]-stats[4])/stats[5]\n",
    "valid_x[:,3]=(valid_x[:,3]-stats[6])/stats[7]\n",
    "k_mean_test=np.zeros(16)\n",
    "valid_y=valid_data[ind,5:]\n",
    "\n",
    "for i in range(len(valid_y)):\n",
    "    valid_y[i,:]=np.log(valid_y[i,:]/np.max(valid_y[i,:]))\n",
    "\n",
    "for i in range(16):\n",
    "    valid_y[:,i]=(valid_y[:,i]-k_mean[i])/k_std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.FloatTensor(x).to(device)\n",
    "y=torch.FloatTensor(y).to(device)\n",
    "\n",
    "valid_x=torch.FloatTensor(valid_x).to(device)\n",
    "valid_y=torch.FloatTensor(valid_y).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `learnKappa_layers` class is a neural network model built using PyTorch's nn.Module. It consists of three fully connected (linear) layers with ReLU activation and dropout for regularization. \n",
    "\n",
    "Define here a 2 hidden-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class learnKappa_layers(nn.Module):\n",
    "    def __init__(self, In_nodes, Hid, Out_nodes):\n",
    "        super(learnKappa_layers, self).__init__()\n",
    "        self.linear1 = nn.Linear(In_nodes, Hid)  # First layer: Input to hidden\n",
    "        self.linear2 = nn.Linear(Hid, Hid)       # Second layer: Hidden to hidden\n",
    "        self.linear3 = nn.Linear(Hid, Out_nodes) # Third layer: Hidden to output\n",
    "        self.dropout = nn.Dropout(0.25)          # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.linear1(x)\n",
    "        h1 = torch.relu(x2)            # ReLU activation for layer 1\n",
    "        h1 = self.dropout(h1)          # Apply dropout\n",
    "        \n",
    "        h2 = self.linear2(h1)\n",
    "        h3 = torch.relu(h2)            # ReLU activation for layer 2\n",
    "        h3 = self.dropout(h3)          # Apply dropout\n",
    "\n",
    "        y_pred = self.linear3(h3)      # Final output layer\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeltrain_loss function trains a neural network model using PyTorch with a custom loss calculation and implements early stopping to prevent overfitting and save computing time.\n",
    "\n",
    "Early Stopping: Stops training if the validation loss does not improve for patience consecutive epochs, saving the best model state. Increasing patience will further reduce loss, but lengthen runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeltrain_loss(In_nodes, Hid, Out_nodes, lr, epochs, x, y, valid_x, valid_y, model, k_std_y, k_mean, k_std, patience=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)  # Adam optimizer\n",
    "    loss_fn = torch.nn.L1Loss(reduction='mean')  # L1 loss for gradient computation\n",
    "    loss_array = torch.zeros([epochs, 3])  # Array to store epoch, train, and validation losses\n",
    "\n",
    "    best_loss = float('inf')  # Initialize the best validation loss as infinity\n",
    "    no_improvement = 0  # Counter for epochs without improvement\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "\n",
    "    # Add a progress bar\n",
    "    with tqdm(total=epochs, desc=\"Training Progress\", unit=\"epoch\") as pbar:\n",
    "        for k in range(epochs):\n",
    "            optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "            y_pred = model(x)  # Forward pass for training data\n",
    "            \n",
    "            valid_pred = model(valid_x)  # Forward pass for validation data\n",
    "            \n",
    "            # Loss used for gradient calculation\n",
    "            loss = loss_fn(y_pred * k_std_y, y * k_std_y)\n",
    "            \n",
    "            loss_train = torch.mean(torch.abs(torch.exp(y_pred * k_std + k_mean) - torch.exp(y * k_std + k_mean)))\n",
    "            loss_valid = torch.mean(torch.abs(torch.exp(valid_pred * k_std + k_mean) - torch.exp(valid_y * k_std + k_mean)))\n",
    "            \n",
    "            loss.backward()  # Backpropagate the gradient\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            # Record the losses for this epoch\n",
    "            loss_array[k, 0] = k  \n",
    "            loss_array[k, 1] = loss_train.item()  \n",
    "            loss_array[k, 2] = loss_valid.item()  \n",
    "\n",
    "            # Update the progress bar with the current epoch and losses\n",
    "            pbar.set_postfix(\n",
    "                train_loss=loss_train.item(), \n",
    "                valid_loss=loss_valid.item(), \n",
    "                patience_count=no_improvement\n",
    "            )\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "            # Early stopping: Check if validation loss improves\n",
    "            if loss_valid.item() < best_loss:\n",
    "                best_loss = loss_valid.item()  # Update best loss\n",
    "                no_improvement = 0\n",
    "                best_model_state = model.state_dict()  \n",
    "            else:\n",
    "                no_improvement += 1  # Increment no improvement counter\n",
    "\n",
    "            # If no improvement for 'patience' epochs, stop training\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {k+1}. Validation loss has not improved for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "            # Free memory by deleting intermediate variables\n",
    "            del loss, y_pred\n",
    "            \n",
    "    # Restore the best model state after training\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, loss_array[:k+1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weighting option here, all weights = 1.0 in default run \n",
    "# Weight settings here\n",
    "kms1=1.0 \n",
    "kms2=1.0\n",
    "# Weight per node set here. Set weights to kms1 or to kms2 to use values above\n",
    "k16 = kms1; k15 = kms1; k14 = kms1; k13 = kms1; k12 = kms1; k11 = kms1; \n",
    "k10 = kms1; k9 = kms1; k8 = kms1; k7 = kms1; k6 = kms1; k5 = kms1; \n",
    "k4 = kms1; k3 = kms1; k2 = kms1; k1 = kms1; \n",
    "# Weight for each layer. \n",
    "# ARRANGED FROM TOP (node = 16) TO BOTTOM (node = 1)\n",
    "kmask=np.array([k16,k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train N1 for the first time!\n",
    "epochs, k_points, lr=3000, 16, 1e-03\n",
    "in_nod, hid_nod, o_nod = 4, 32, 16\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "k_mean_c=torch.tensor(k_mean).float().to(device)\n",
    "k_std_c=torch.tensor(k_std).float().to(device)\n",
    "\n",
    "model = learnKappa_layers(in_nod, hid_nod, o_nod)\n",
    "model = model.to(device)\n",
    "model, loss_array = modeltrain_loss(in_nod, hid_nod, o_nod, lr, epochs, x, y,valid_x,valid_y,model,torch.tensor(kmask).float().to(device), k_mean_c, k_std_c)\n",
    "\n",
    "### in this version, there is no weighting in the loss function. This will be added in section 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced training/validation loss plot\n",
    "plot_training_validation_loss(loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce here Figure 4 of Sane et al (2023) with enhanced visualizations\n",
    "As in the training data from GOTM\n",
    "- Node 1 is adjacent to BOTTOM of OBSL\n",
    "- Node 16 is adjacent to TOP of OBSL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced performance visualization\n",
    "plot_enhanced_performance(model, x, valid_x, y, valid_y, k_mean, k_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Modifying Loss Function with Node-Specific Weighting\n",
    "\n",
    "As discussed at the end of section 3.4 of Sane et al. (2023), we adjust the training process by modifying the loss function to prioritize nodes with higher variance. Key steps and configurations:\n",
    "\n",
    "**Node-Specific Weighting**: We applied node-specific weights (`kmask`) to amplify the loss values for nodes 8 to 13 by a factor of 100 (`kms2 = 100.0`), while keeping the weight at other nodes as 1.0 (`kms1 = 1.0`). \n",
    "\n",
    "This approach helps focus the model on reducing errors for nodes with higher variance, making them easier to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weighting option here \n",
    "# Weight settings here\n",
    "kms1=1.0 \n",
    "kms2=100.0\n",
    "# Weight per node set here. Set weights to kms1 or to kms2 to use values above\n",
    "k16 = kms1; k15 = kms1; k14 = kms1; k13 = kms2; k12 = kms2; k11 = kms2; \n",
    "k10 = kms2; k9 = kms2; k8 = kms2; k7 = kms1; k6 = kms1; k5 = kms1; \n",
    "k4 = kms1; k3 = kms1; k2 = kms1; k1 = kms1; \n",
    "# Weight for each layer. \n",
    "# ARRANGED FROM TOP (node = 16) TO BOTTOM (node = 1)\n",
    "kmask=np.array([k16,k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, k_points, lr=3000, 16, 1e-03\n",
    "\n",
    "# Loss weighting implemented here\n",
    "in_nod, hid_nod, o_nod = 4, 32, 16\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "k_mean_c=torch.tensor(k_mean).float().to(device)\n",
    "k_std_c=torch.tensor(k_std).float().to(device)\n",
    "\n",
    "model = learnKappa_layers(in_nod, hid_nod, o_nod)\n",
    "model = model.to(device)\n",
    "model, loss_array = modeltrain_loss(in_nod, hid_nod, o_nod, lr, epochs, x, y,valid_x,valid_y,model,torch.tensor(kmask).float().to(device), k_mean_c, k_std_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the weighted model performance with enhanced visualizations\n",
    "plot_enhanced_performance(model, x, valid_x, y, valid_y, k_mean, k_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will have to look closely to find differences from the unweighted training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Hyperparameter Sweep for Network Architecture\n",
    "\n",
    "In this section, we conducted a hyperparameter sweep to evaluate the performance of different neural network architectures by varying:\n",
    "- The **number of layers**.\n",
    "- The **number of hidden nodes per layer**.\n",
    "\n",
    "#### Parameter Count Calculation\n",
    "The number of trainable parameters for a given architecture is calculated as:\n",
    "$$\n",
    "P = \\sum_{l=1}^{L} \\big( n_{\\text{input}, l} \\times n_{\\text{output}, l} + n_{\\text{output}, l} \\big)\n",
    "$$\n",
    "Where:\n",
    "- $ n_{\\text{input}, l} $: Number of input nodes to layer $l $.\n",
    "- $ n_{\\text{output}, l} $: Number of output nodes from layer $ l$.\n",
    "\n",
    "Using a **4-core CPU with 32GB RAM**, the estimated computational time for training all models with the above configurations is many hours. \n",
    "\n",
    "- **We recommend using a GPU server.**  \n",
    "\n",
    "- In addition, limit sweep to **1-2 layers** and **2-64 hidden nodes** per layer. This enables a much faster, but still meaningful evaluation of the model's depth and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weighting option here, all weights = 1.0 in hyperparameter sweep\n",
    "# Weight settings here\n",
    "kms1=1.0 \n",
    "kms2=1.0\n",
    "# Weight per node set here. Set weights to kms1 or to kms2 to use values above\n",
    "k16 = kms1; k15 = kms1; k14 = kms1; k13 = kms1; k12 = kms1; k11 = kms1; \n",
    "k10 = kms1; k9 = kms1; k8 = kms1; k7 = kms1; k6 = kms1; k5 = kms1; \n",
    "k4 = kms1; k3 = kms1; k2 = kms1; k1 = kms1; \n",
    "# Weight for each layer. \n",
    "# ARRANGED FROM TOP (node = 16) TO BOTTOM (node = 1)\n",
    "kmask=np.array([k16,k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform hyper-parameter sweep in this section.\n",
    "\n",
    "epochs, k_points, lr=3000, 16, 1e-03\n",
    "\n",
    "hid_array=np.array([2,4,8,16,32,64])\n",
    "\n",
    "lays = np.array([1,2])\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "k_mean_c=torch.tensor(k_mean).float().to(device)\n",
    "k_std_c=torch.tensor(k_std).float().to(device)\n",
    "\n",
    "first_print_done = False  # Ensure the success message prints only once\n",
    "\n",
    "for la in lays:\n",
    "    cwd_sd = cwd_output + 'ensemble_models_layers' + str(la) + '/'\n",
    "\n",
    "    for h in hid_array:\n",
    "        in_nod, hid_nod, o_nod = 4, h, 16\n",
    "        print('la, h is >', la, h)\n",
    "\n",
    "        if la == 1:\n",
    "            model = ff.learnKappa_layers1(in_nod, hid_nod, o_nod)\n",
    "        elif la == 2:\n",
    "            model = ff.learnKappa_layers2(in_nod, hid_nod, o_nod)\n",
    "        elif la == 3:\n",
    "            model = ff.learnKappa_layers3(in_nod, hid_nod, o_nod)\n",
    "        elif la == 4:\n",
    "            model = ff.learnKappa_layers4(in_nod, hid_nod, o_nod)\n",
    "        else:\n",
    "            print('Check code')\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model, loss_array = modeltrain_loss(in_nod, hid_nod, o_nod, lr, epochs, x, y, valid_x, valid_y, model, \n",
    "                                            torch.tensor(kmask).float().to(device), k_mean_c, k_std_c)\n",
    "\n",
    "        directory = os.path.join(cwd_sd, f'mod_dir_{h}')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        model_path = os.path.join(directory, 'model.pt')\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "        loss_path = os.path.join(directory, 'loss_array.txt')\n",
    "        np.savetxt(loss_path, loss_array.detach().numpy())\n",
    "\n",
    "        if not first_print_done:\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"Model saved successfully: {model_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to save model: {model_path}\")\n",
    "\n",
    "            if os.path.exists(loss_path):\n",
    "                print(f\"Loss array saved successfully: {loss_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to save loss array: {loss_path}\")\n",
    "\n",
    "            first_print_done = True  \n",
    "\n",
    "        del model, loss_array\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncertainty in training 4-32-32-16 model: \n",
    "## this is 4 input + 2 layers with 32 hidden nodes in each layer + 16 output nodes  \n",
    "ensemble_dir = cwd_output + 'ensemble_models_layers2_uncertainty'\n",
    "os.makedirs(ensemble_dir, exist_ok=True)\n",
    "\n",
    "epochs, k_points, lr = 3000, 16, 1e-03\n",
    "in_nod, hid_nod, o_nod = 4, 32, 16\n",
    "\n",
    "seeds = np.arange(10, 15)\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Training with seed {seed}\")\n",
    "    # Define model directory for this seed\n",
    "    model_dir = os.path.join(ensemble_dir, f'mod_dir_{seed}')\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model = ff.learnKappa_layers2(in_nod, hid_nod, o_nod)  # Update layers as needed\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    model, loss_array = modeltrain_loss(\n",
    "        in_nod, hid_nod, o_nod, lr, epochs, x, y, valid_x, valid_y, model,\n",
    "        torch.tensor(kmask).float().to(device), k_mean_c, k_std_c\n",
    "    )\n",
    "\n",
    "    # Save model and loss array for this seed\n",
    "    torch.save(model, os.path.join(model_dir, 'model.pt'))\n",
    "    np.savetxt(os.path.join(model_dir, 'loss_array.txt'), loss_array.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced visualization of the hyperparameter sweep results\n",
    "score_eval(hid_array, lays, valid_x, valid_y, k_mean_c, k_std_c, cwd_output)\n",
    "save_losses_by_seed(os.path.join(cwd_output, 'ensemble_models_layers2_uncertainty/'), seeds)\n",
    "\n",
    "# Load the scores for visualization\n",
    "scores_path = os.path.join(cwd_output, 'n1scoredata/N1scores.txt')\n",
    "if os.path.exists(scores_path):\n",
    "    scores = np.loadtxt(scores_path)\n",
    "    # Create enhanced hyperparameter comparison plot\n",
    "    plot_hyperparameter_comparison(scores, hid_array, lays, loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation and Discussion of Hyperparameter Sweep and Uncertainty Evaluation \n",
    "To identify the optimal hyperparameters for the model, we evaluate different configurations based on their **linear correlation coefficient** performance. The process is as follows:\n",
    "\n",
    "- Hyperparameter Variation: We explore different numbers of hidden layers and nodes per layer. Each configuration is used to train the model, producing predictions for 16 $g(\\sigma)$ points (denoted as $y$).\n",
    "\n",
    "- Prediction Post-Processing: Each predicted $y$ is denormalized to restore its original scale. Extreme values (beyond the 5th-95th percentile) are removed to mitigate the influence of outliers, reduced the impact of noisy predictions. The Pearson correlation between the filtered predictions and the ground truth $y$ is computed.\n",
    "\n",
    "- Final Correlation Computation: The mean correlation across all 16 $g(\\sigma)$ points is taken as the final evaluation score for each hyperparameter set. This is plotted in (a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter sweep, plot (a) \n",
    "Plot (a) shows that 2 layers with 64 nodes achieved the best performance under the linear correlation coefficient metric. \n",
    "\n",
    "**At 2 layers with 32 nodes, performance starts to asymptote. This is the structure selected in Sane et al. (2023) to balance skill and with computational cost.** \n",
    "\n",
    "Sane et al. (2023) Figure 3(a) shows a search up to 4 layers with 512 nodes. To reduce computational cost, we cut off the sweep at 2 layers with 64 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 loss with uncertainty, plot (b)\n",
    "Plot (b), for 2 layers with 32 nodes, we make an addition to Sane et al. (2023) by estimating uncertainty in model predictions by training with different random seeds. This captures variations in initialization. \n",
    "\n",
    "We use 5 random seeds to balance computational efficiency and result reliability. Increasing the number of seeds would provide a more robust estimation of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering analysis\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a copy of the data for clustering\n",
    "data_for_clustering = copy.deepcopy(data_load_main)\n",
    "\n",
    "# Extract input features for clustering\n",
    "features = data_for_clustering[:, :4].astype(np.float64)  # f, B0, u*, h\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Apply K-means clustering with 4 clusters\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_assignments = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Print cluster sizes\n",
    "for i in range(n_clusters):\n",
    "    count = np.sum(cluster_assignments == i)\n",
    "    print(f\"Cluster {i}: {count} samples ({count/len(cluster_assignments)*100:.2f}%)\")\n",
    "\n",
    "# Visualize the clusters with the enhanced plot\n",
    "plot_cluster_analysis(data_for_clustering, cluster_assignments, n_clusters)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}