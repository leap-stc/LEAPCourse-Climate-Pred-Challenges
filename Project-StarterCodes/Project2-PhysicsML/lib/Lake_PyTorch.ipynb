{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lake_PyTorch_merged.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leap-stc/LEAPCourse-Climate-Pred-Challenges/blob/main/Project-StarterCodes/Project2-PhysicsML/lib/Lake_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Lake Thermal Stratification"
      ],
      "metadata": {
        "id": "fHQJ6omnn5rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we reproduce physics-guided deep learning models proposed in Read *et al* (2019) and Jia *et al* (2019). We use only the Lake Mendota data in this notebook. \n",
        "\n",
        "Both papers combine state-of-the-art deep learning models with strategies to incorporaate \"guidance\" from known physics that governs the dynamics of lake thermal stratefication. \n",
        "\n",
        "This notebook provides a set of starter codes for [LEAP CPC Spring 2022](https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges)[Project 2](https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges/tree/main/Project-StarterCodes/Project2-PhysicsML) on\n",
        "\n",
        "+ how to download data,\n",
        "+ how to set up physics-guided loss functions,\n",
        "+ how to setup deep learning models using PyTorch,\n",
        "+ how to pre-train and re-train DL models, with early stopping,\n",
        "+ how to evaluate and visualize model predictions.\n",
        "\n",
        "All the codes in this notebook can be modified to implement research ideas. \n",
        "\n",
        "To start, the **team leader** of each team should \n",
        "+ create in the course folder a folder for project 2.\n",
        "This folder can be used to share notes, data, outputs and codes. See [project 2 description](https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges/blob/main/Project-StarterCodes/Project2-PhysicsML/doc/project2_desc.md) for a suggested set up. \n",
        "+ share project 2 folder with all team members.\n",
        "+ team members should add this folder to their drive by creating a shortcut to this shared project folder in their own folder for LEAP CPC.\n",
        "+ go to \"File/Save A Copy in Drive/\" (upper left) and save a copy for your team in the project 2 folder that was just created, under an appropriate subfolder. \n",
        "+ for collaboration, team members should all add a shortcut of their project 2 folder to the folder \"Colab Notebooks\" in the root of your Google Drive. See below for more instructions. \n"
      ],
      "metadata": {
        "id": "E_w7noZcoEi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0. Setup Workspace"
      ],
      "metadata": {
        "id": "gwEmKptsAeza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages and adjust settings"
      ],
      "metadata": {
        "id": "j3ihmc5LAu-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install sciencebasepy"
      ],
      "metadata": {
        "id": "lYW9f-Db6Ky-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import torch\n",
        "import cv2\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import sciencebasepy\n",
        "import urllib.request\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from google.colab import drive\n",
        "from sklearn import preprocessing\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "LEkxFV1RArkN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, it is important for the team to set up a shared Google Drive to collaborate using the same notebook, while saving intermediate outputs and results to the shared folder. Therefore"
      ],
      "metadata": {
        "id": "DTbv5k_PxfSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWEy0x4GxwrF",
        "outputId": "c1b51ed4-dd01-4753-a039-34bc96f6f283"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we will be using PyTorch, which will benefit tremendously from acceleration by GPU. Make sure that you are on a GPU run time. This notebook runs sufficiently fast without a [Google Colab Pro](https://colab.research.google.com/signup) subscription. However, teams with ambitious plans may need more memory and more computational power offer by a Pro subscription. The Pro subscription is not available to LionMail. You would need to share your project with a personal gmail account to enable the Pro subscription. "
      ],
      "metadata": {
        "id": "1aNurNU_x1ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm0iYiXex3_e",
        "outputId": "bae2e6b0-bd50-4c14-f82e-289891dd5b59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Data Directory and Shared Folder\n",
        "\n",
        "Each team should create a shortcut to your project folder in the \"Colab Notebooks\" folder similar to \"LEAP Colab Notebooks/Project 2/.\" There should be a folder for raw data and a folder for outputs. \n",
        "\n",
        "**data_dir** should be set as 'drive/Mydrive/Colab Notebooks/SHARED_FOLDER/Data/', where SHARED_FOLDER is the name of the shared folder."
      ],
      "metadata": {
        "id": "dES7sUeVNn5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to add shortcut of the shared folder to folder \"/MyDrive/Colab Notebooks/\"\n",
        "# and change the folder name accordingly\n",
        "data_dir = 'drive/MyDrive/Colab Notebooks/LEAP Colab Notebooks/Project 2/Data/'\n",
        "output_dir = 'drive/MyDrive/Colab Notebooks/LEAP Colab Notebooks/Project 2/Output/'\n",
        "if not os.path.isdir(data_dir): os.mkdir(data_dir) ## create the folder\n",
        "if not os.path.isdir(output_dir): os.mkdir(output_dir) ## create the folder"
      ],
      "metadata": {
        "id": "4gZPT-2qlzTQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Unzip Data\n",
        "\n",
        "This block will only download the data to the shared Google Drive Folder once. By having a designated folder for raw data, you can always delete every files from the folder and re-download all data to start over, in the case some files are overwritten inadvertently. "
      ],
      "metadata": {
        "id": "9u4XWc4xT1Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.listdir(data_dir) == []: ## data not downloaded yet\n",
        "    print('Data folder is empty! Download the files now!')\n",
        "    # set the url\n",
        "    zipurl = 'https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges/raw/main/Project-StarterCodes/Project2-PhysicsML/data/numpy_files.zip'\n",
        "    # download the file from the URL\n",
        "    zipresp = urlopen(zipurl)\n",
        "    # create a new file on the hard drive\n",
        "    tempzip = open(data_dir + 'numpy_files.zip', \"wb\")\n",
        "    # write the contents of the downloaded file into the new file\n",
        "    tempzip.write(zipresp.read())\n",
        "    # close the newly-created file\n",
        "    tempzip.close()\n",
        "    # re-open the newly-created file with ZipFile()\n",
        "    zf = ZipFile(data_dir + 'numpy_files.zip')\n",
        "    # extract its contents into <extraction_path>\n",
        "    # note that extractall will automatically create the path\n",
        "    zf.extractall(path = data_dir)\n",
        "    # close the ZipFile instance\n",
        "    zf.close()\n",
        "    print('Files all downloaded!')"
      ],
      "metadata": {
        "id": "6aVvhVDFnmOr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I. Model Pre-training\n",
        "\n",
        "[Data](https://www.sciencebase.gov/catalog/item/5d88ea50e4b0c4f70d0ab3c0) and [original codes](https://zenodo.org/record/3497495#.YgB85_XMIqv) of Read et al (2019) are all publicly available, which makes this reproducibility effort reasonably easy. \n",
        "\n",
        "Read et al (2019) and Jia et al (2019) proposed a [Long short-term memory (LSTM)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) network for modeling temperature dynamics. The original codes used TensorFlow version 1.0. In this notebook, we adopt this model using [PyTorch](https://pytorch.org/), an open-source ML framework with excellent libraries for deep learning.  \n",
        "\n",
        "As proposed in the papers, physics guidance were introduced in two ways. The first is adding an energy conservation term to the loss funcation. The second is pretraining the LSTM networks using simulation data from the [General Lake Model](https://aed.see.uwa.edu.au/research/models/glm/). \n",
        "\n",
        "In the following, we outline\n",
        "+ Algorithm configering constants\n",
        "+ Set up functions for calculating energy conservation\n",
        "+ Load the data\n",
        "+ Setup the LSTM model\n",
        "+ Train the LSTM model using simulated data"
      ],
      "metadata": {
        "id": "CyVLqEU1iDrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n",
        "We set the algorithm configuring constants in this part."
      ],
      "metadata": {
        "id": "t2WP4BCOjM5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for LSTM\n",
        "epochs = 300 ## number of max training epochs\n",
        "batch_size = 100 ## batch size in SGD, must be multiplication of n_depths\n",
        "learning_rate = 1e-2 ## learning rate \n",
        "state_size = 7 ## dimension of hidden layers \n",
        "input_size = 9 ## matches with the data, last dimension of x\n",
        "phy_size = 10 ## matches with the data, last dimension of phy\n",
        "elam = 0.005 ## loss weight\n",
        "patience = 3 ## patience in early stopping\n",
        "\n",
        "\n",
        "# params for data transformation\n",
        "npic = 10 #16 #40 stride size\n",
        "N_sec = (npic-1)*2+1 ## window width\n",
        "n_depths = 50 ## matches with data, first dimension of x, also = len(depth_areas)\n",
        "ec_threshold = 24\n",
        "depth_areas = torch.Tensor([\n",
        "        39865825,38308175,38308175,35178625,35178625,33403850,31530150,31530150,30154150,30154150,29022000,\n",
        "        29022000,28063625,28063625,27501875,26744500,26744500,26084050,26084050,25310550,24685650,24685650,\n",
        "        23789125,23789125,22829450,22829450,21563875,21563875,20081675,18989925,18989925,17240525,17240525,\n",
        "        15659325,14100275,14100275,12271400,12271400,9962525,9962525,7777250,7777250,5956775,4039800,4039800,\n",
        "        2560125,2560125,820925,820925,216125]).to(device)\n"
      ],
      "metadata": {
        "id": "b6tnrsyhBCWl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definitions for calculating ec loss"
      ],
      "metadata": {
        "id": "oj9gJRuvFA_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformTempToDensity(temp):\n",
        "    # converts temperature to density\n",
        "    # parameter:\n",
        "        # @temp: single value or array of temperatures to be transformed\n",
        "    densities = 1000 * (1 - ((temp + 288.9414) * (temp - 3.9863)**2) / (508929.2 * (temp + 68.12963)))\n",
        "    return densities\n",
        "\n",
        "def calculate_lake_energy(temps, densities, depth_areas):\n",
        "    # calculate the total energy of the lake for every timestep\n",
        "    # sum over all layers the (depth cross-sectional area)*temp*density*layer_height)\n",
        "    # then multiply by the specific heat of water \n",
        "    dz = 0.5 # thickness for each layer, hardcoded for now\n",
        "    cw = 4186 # specific heat of water\n",
        "    depth_areas = torch.reshape(depth_areas, (-1, 1))\n",
        "    energy = torch.sum(depth_areas * temps * densities * dz * cw, axis=0)\n",
        "    return energy\n",
        "\n",
        "def calculate_lake_energy_deltas(energies, combine_days, surface_area):\n",
        "    # given a time series of energies, compute and return the differences\n",
        "    # between each time step, or time step interval (parameter @combine_days)\n",
        "    # as specified by parameter @combine_days\n",
        "    time = 86400 #seconds per day\n",
        "    energy_deltas = (energies[1:] - energies[:-1]) / (time * surface_area)\n",
        "    return energy_deltas\n",
        "\n",
        "def calculate_vapour_pressure_saturated(temp):\n",
        "    # returns in miilibars\n",
        "    # Converted pow function to exp function workaround pytorch not having autograd implemented for pow\n",
        "    exponent = (9.28603523 - (2332.37885 / (temp + 273.15))) * np.log(10)\n",
        "    return torch.exp(exponent)\n",
        "\n",
        "def calculate_vapour_pressure_air(rel_hum, temp):\n",
        "    rh_scaling_factor = 1\n",
        "    return rh_scaling_factor * (rel_hum / 100) * calculate_vapour_pressure_saturated(temp)\n",
        "\n",
        "\n",
        "def calculate_wind_speed_10m(ws, ref_height = 2.):\n",
        "    # from GLM code glm_surface.c\n",
        "    c_z0 = torch.tensor(0.001) #default roughness\n",
        "    return ws * (torch.log(10.0 / c_z0) / torch.log(ref_height / c_z0))\n",
        "\n",
        "def calculate_air_density(air_temp, rh):\n",
        "    # returns air density in kg / m^3\n",
        "    # equation from page 13 GLM/GLEON paper(et al Hipsey)\n",
        "    # Ratio of the molecular (or molar) weight of water to dry air\n",
        "    mwrw2a = 18.016 / 28.966\n",
        "    c_gas = 1.0e3 * 8.31436 / 28.966\n",
        "\n",
        "    # atmospheric pressure\n",
        "    p = 1013. #mb\n",
        "\n",
        "    # water vapor pressure\n",
        "    vapPressure = calculate_vapour_pressure_air(rh, air_temp)\n",
        "\n",
        "    # water vapor mixing ratio (from GLM code glm_surface.c)\n",
        "    r = mwrw2a * vapPressure / (p - vapPressure)\n",
        "    return (1.0 / c_gas * (1 + r)/(1 + r / mwrw2a) * p / (air_temp + 273.15)) * 100\n",
        "\n",
        "def calculate_heat_flux_sensible(surf_temp, air_temp, rel_hum, wind_speed):\n",
        "    # equation 22 in GLM/GLEON paper(et al Hipsey)\n",
        "    # GLM code ->  Q_sensibleheat = -CH * (rho_air * 1005.) * WindSp * (Lake[surfLayer].Temp - MetData.AirTemp);\n",
        "    # calculate air density \n",
        "    rho_a = calculate_air_density(air_temp, rel_hum)\n",
        "\n",
        "    # specific heat capacity of air in J/(kg*C)\n",
        "    c_a = 1005.\n",
        "\n",
        "    # bulk aerodynamic coefficient for sensible heat transfer\n",
        "    c_H = 0.0013\n",
        "\n",
        "    # wind speed at 10m\n",
        "    U_10 = calculate_wind_speed_10m(wind_speed)\n",
        "    return -rho_a * c_a * c_H * U_10 * (surf_temp - air_temp)\n",
        "\n",
        "def calculate_heat_flux_latent(surf_temp, air_temp, rel_hum, wind_speed):\n",
        "    # equation 23 in GLM/GLEON paper(et al Hipsey)\n",
        "    # GLM code-> Q_latentheat = -CE * rho_air * Latent_Heat_Evap * (0.622/p_atm) * WindSp * (SatVap_surface - MetData.SatVapDef)\n",
        "    # where,         SatVap_surface = saturated_vapour(Lake[surfLayer].Temp);\n",
        "    #                rho_air = atm_density(p_atm*100.0,MetData.SatVapDef,MetData.AirTemp);\n",
        "    # air density in kg/m^3\n",
        "    rho_a = calculate_air_density(air_temp, rel_hum)\n",
        "\n",
        "    # bulk aerodynamic coefficient for latent heat transfer\n",
        "    c_E = 0.0013\n",
        "\n",
        "    # latent heat of vaporization (J/kg)\n",
        "    lambda_v = 2.453e6\n",
        "\n",
        "    # wind speed at 10m height\n",
        "    # U_10 = wind_speed\n",
        "    U_10 = calculate_wind_speed_10m(wind_speed)\n",
        "    # \n",
        "    # ratio of molecular weight of water to that of dry air\n",
        "    omega = 0.622\n",
        "\n",
        "    # air pressure in mb\n",
        "    p = 1013.\n",
        "\n",
        "    e_s = calculate_vapour_pressure_saturated(surf_temp)\n",
        "    e_a = calculate_vapour_pressure_air(rel_hum, air_temp)\n",
        "    return -rho_a * c_E * lambda_v * U_10 * (omega / p) * (e_s - e_a)\n",
        "\n",
        "def calculate_energy_fluxes(phys, surf_temps, combine_days):    \n",
        "    e_s = 0.985 # emissivity of water, given by Jordan\n",
        "    alpha_sw = 0.07 # shortwave albedo, given by Jordan Read\n",
        "    alpha_lw = 0.03 # longwave, albeda, given by Jordan Read\n",
        "    sigma = 5.67e-8 # Stefan-Baltzmann constant\n",
        "    R_sw_arr = phys[:-1,2] + (phys[1:,2] - phys[:-1,2]) / 2\n",
        "    R_lw_arr = phys[:-1,3] + (phys[1:,3] - phys[:-1,3]) / 2\n",
        "    R_lw_out_arr = e_s * sigma * (torch.pow(surf_temps[:] + 273.15, 4))\n",
        "    R_lw_out_arr = R_lw_out_arr[:-1] + (R_lw_out_arr[1:] - R_lw_out_arr[:-1]) / 2\n",
        "\n",
        "    air_temp = phys[:-1,4] \n",
        "    air_temp2 = phys[1:,4]\n",
        "    rel_hum = phys[:-1,5]\n",
        "    rel_hum2 = phys[1:,5]\n",
        "    ws = phys[:-1, 6]\n",
        "    ws2 = phys[1:,6]\n",
        "    t_s = surf_temps[:-1]\n",
        "    t_s2 = surf_temps[1:]\n",
        "    E = calculate_heat_flux_latent(t_s, air_temp, rel_hum, ws)\n",
        "    H = calculate_heat_flux_sensible(t_s, air_temp, rel_hum, ws)\n",
        "    E2 = calculate_heat_flux_latent(t_s2, air_temp2, rel_hum2, ws2)\n",
        "    H2 = calculate_heat_flux_sensible(t_s2, air_temp2, rel_hum2, ws2)\n",
        "    E = (E + E2) / 2\n",
        "    H = (H + H2) / 2\n",
        "    fluxes = (R_sw_arr[:-1] * (1-alpha_sw) + R_lw_arr[:-1] * (1-alpha_lw) - R_lw_out_arr[:-1] + E[:-1] + H[:-1])\n",
        "    return fluxes\n",
        "\n",
        "def calculate_ec_loss(inputs, outputs, phys, depth_areas, n_depths, ec_threshold, combine_days=1):\n",
        "    # description: calculates energy conservation loss\n",
        "    # parameters: \n",
        "        # @inputs: features\n",
        "        # @outputs: labels\n",
        "        # @phys: features(not standardized) of sw_radiation, lw_radiation, etc\n",
        "        # @ labels modeled temp (will not used in loss, only for test) !!! DO NOT MODIFY THIS LINE !!!\n",
        "        # @depth_areas: cross-sectional area of each depth\n",
        "        # @n_depths: number of depths\n",
        "        # @use_gpu: gpu flag\n",
        "        # @combine_days: how many days to look back to see if energy is conserved\n",
        "    #*********************************************************************************\n",
        "    n_sets = int(inputs.shape[0] / n_depths)\n",
        "    densities = transformTempToDensity(outputs)\n",
        "    diff_per_set_r = torch.empty(n_sets) \n",
        "    for i in range(n_sets):\n",
        "        # loop through sets of n_depths\n",
        "        # indices\n",
        "        start_index = i * n_depths\n",
        "        end_index = (i + 1) * n_depths\n",
        "        # calculate lake energy for each timestep\n",
        "        lake_energies = calculate_lake_energy(outputs[start_index:end_index, :], densities[start_index:end_index, :], depth_areas)\n",
        "        # calculate energy change in each timestep\n",
        "        lake_energy_deltas = calculate_lake_energy_deltas(lake_energies, combine_days, depth_areas[0])\n",
        "        lake_energy_deltas = lake_energy_deltas[1:]\n",
        "        # calculate sum of energy flux into or out of the lake at each timestep\n",
        "        lake_energy_fluxes = calculate_energy_fluxes(phys[start_index, :, :], outputs[start_index, :], combine_days)\n",
        "        ### can use this to plot energy delta and flux over time to see if they line up\n",
        "        diff_vec = torch.abs(lake_energy_deltas - lake_energy_fluxes) \n",
        "        tmp_mask = 1 - phys[start_index+1, 1:-1, 9] \n",
        "        tmp_loss = torch.mean(diff_vec * tmp_mask)\n",
        "        diff_per_set_r[i] = tmp_loss \n",
        "    diff_per_set = torch.clamp(diff_per_set_r - ec_threshold, min=0, max=999999)\n",
        "    return torch.mean(diff_per_set), diff_vec, diff_per_set_r, diff_per_set \n"
      ],
      "metadata": {
        "id": "b144t0gHFAEA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "\n",
        "Here we load the data for pretraining and partition the data into training, validation and test sets. "
      ],
      "metadata": {
        "id": "NNygPr93qYCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "x_full = np.load(data_dir + '/processed_features.npy') #standardized inputs\n",
        "x_raw_full = np.load(data_dir + '/features.npy') #raw inputs\n",
        "diag_full = np.load(data_dir + '/diag.npy') \n",
        "label_full = np.load(data_dir + '/labels.npy') #simulated lake temperatures\n",
        "\n",
        "# process data\n",
        "mask_full = np.ones(label_full.shape) # no missing values to mask for simulated data\n",
        "phy_full = np.concatenate((x_raw_full[:,:,:(-2)], diag_full), axis=2) \n",
        "## phy: 4-air temp, 5-rel hum, 6-wind speed, 9-ice flag\n",
        "\n",
        "# print the shape\n",
        "print(f\"The shape of x_full is {np.shape(x_full)}\")\n",
        "print(f\"The shape of x_raw_full is {np.shape(x_raw_full)}\")\n",
        "print(f\"The shape of diag_full is {np.shape(diag_full)}\")\n",
        "print(f\"The shape of label is {np.shape(label_full)}\")\n",
        "\n",
        "# train-val split\n",
        "## in model pretraining, we use validation set to find the best number of steps \n",
        "## Here we use equal size for train-val split\n",
        "## They can be of different sizes by using different n_steps \n",
        "N = np.shape(x_full)[1] # the total number of samples\n",
        "idx_tr, idx_va, idx_te = (int(N/3), int(N/3*2), N)\n",
        "\n",
        "\n",
        "## training\n",
        "x_tr = x_full[:, :idx_tr]\n",
        "y_tr = label_full[:, :idx_tr]\n",
        "p_tr = phy_full[:, :idx_tr]\n",
        "m_tr = mask_full[:, :idx_tr]\n",
        "## validation\n",
        "x_va = x_full[:, idx_tr:idx_va]\n",
        "y_va = label_full[:, idx_tr:idx_va]\n",
        "p_va = phy_full[:, idx_tr:idx_va]\n",
        "m_va = mask_full[:, idx_tr:idx_va]\n",
        "\n",
        "# create data\n",
        "n_steps = int(idx_tr/npic)\n",
        "x_train = np.zeros([n_depths * N_sec, n_steps, input_size])\n",
        "y_train = np.zeros([n_depths * N_sec, n_steps])\n",
        "p_train = np.zeros([n_depths * N_sec, n_steps, phy_size])\n",
        "m_train = np.zeros([n_depths * N_sec, n_steps])\n",
        "\n",
        "x_val = np.zeros([n_depths * N_sec, n_steps, input_size])\n",
        "y_val = np.zeros([n_depths * N_sec, n_steps])\n",
        "p_val = np.zeros([n_depths * N_sec, n_steps, phy_size])\n",
        "m_val = np.zeros([n_depths * N_sec, n_steps])\n",
        "\n",
        "\n",
        "for i in range(1, N_sec + 1):\n",
        "    r_1 = int((i - 1) * n_steps / 2) ## stride size = n_steps / 2\n",
        "    r_2 = r_1 + n_steps ## step size = n_step\n",
        "    x_train[(i-1)*n_depths:(i*n_depths)] = x_tr[:, r_1:r_2]\n",
        "    y_train[(i-1)*n_depths:(i*n_depths)] = y_tr[:, r_1:r_2]\n",
        "    p_train[(i-1)*n_depths:(i*n_depths)] = p_tr[:, r_1:r_2]\n",
        "    m_train[(i-1)*n_depths:(i*n_depths)] = m_tr[:, r_1:r_2]\n",
        "    x_val[(i-1)*n_depths:(i*n_depths)] = x_va[:, r_1:r_2]\n",
        "    y_val[(i-1)*n_depths:(i*n_depths)] = y_va[:, r_1:r_2]\n",
        "    p_val[(i-1)*n_depths:(i*n_depths)] = p_va[:, r_1:r_2]\n",
        "    m_val[(i-1)*n_depths:(i*n_depths)] = m_va[:, r_1:r_2]\n",
        "\n",
        "x_f = np.concatenate((x_train, x_val), axis=0)\n",
        "y_f = np.concatenate((y_train, y_val), axis=0)\n",
        "p_f = np.concatenate((p_train, p_val), axis=0)\n",
        "m_f = np.concatenate((m_train, m_val), axis=0)\n",
        "\n",
        "# print the shape\n",
        "print(f\"The shape of x_train is {np.shape(x_train)}\")\n",
        "print(f\"The shape of y_train is {np.shape(y_train)}\")\n",
        "print(f\"The shape of p_train is {np.shape(p_train)}\")\n",
        "print(f\"The shape of m_train is {np.shape(m_train)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmNNJ6lhDSG8",
        "outputId": "2c8a002b-6a72-4298-c665-f7bd1877c4f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of x_full is (50, 12691, 9)\n",
            "The shape of x_raw_full is (50, 12691, 9)\n",
            "The shape of diag_full is (50, 12691, 3)\n",
            "The shape of label is (50, 12691)\n",
            "The shape of x_train is (950, 423, 9)\n",
            "The shape of y_train is (950, 423)\n",
            "The shape of p_train is (950, 423, 10)\n",
            "The shape of m_train is (950, 423)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class definition of LSTM model\n",
        "We set up a relatively standard LSTM network. See [the PyTorch tutorial on LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) for details. "
      ],
      "metadata": {
        "id": "HlT2ADUu4Ose"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMnet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, n_layers):\n",
        "        super(LSTMnet, self).__init__()\n",
        "        self.output_size = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_size = input_dim, \n",
        "                            hidden_size = hidden_dim, \n",
        "                            num_layers = n_layers, \n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        lstm_out, hidden = self.lstm(x)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(lstm_out)\n",
        "        out = out.view(batch_size, -1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ETRrJq8n_p97"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definitions for Deep Learning\n",
        "\n",
        "In the training, the loss funcation is a weighted sum of the *prediction error* loss and *energy conservation* loss. The algorithm processes a mini batch of data at a time and use backpropagation to update model parameters. The training intends to run the specified number of epochs but will stop *early* if the returned loss increases for three consequential updates. "
      ],
      "metadata": {
        "id": "YXSkSUSRmm0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_rmse_loss(input, target, weight):\n",
        "    # defined weighted rmse loss\n",
        "    # used in model training\n",
        "    return torch.sqrt(torch.sum(weight * (input - target) ** 2) / torch.sum(weight))\n",
        "\n",
        "def batch_sample_generator(size_list, batch_size):\n",
        "    ## create advanced batches for training-val-test\n",
        "    ## each batch should match with depth_areas for calculating ec_loss\n",
        "    n_batch = int(size_list / batch_size)\n",
        "    n_remain = size_list % batch_size\n",
        "    batch_sampler = [list(range(i * batch_size, i * batch_size + batch_size)) for i in range(n_batch)]\n",
        "    if n_remain > 0:\n",
        "        batch_sampler += [list(range(n_batch * batch_size, n_batch * batch_size + n_remain))]\n",
        "    return batch_sampler\n",
        "\n",
        "\n",
        "def train(model, epochs, optimizer, train_loader, valid_loader, early_stopping=True):\n",
        "    the_last_loss = 100\n",
        "    trigger_times = 0\n",
        "    for i in range(epochs):\n",
        "        print('This is epoch ' + str(i + 1))\n",
        "        for batch, (x, y, m, p) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            size = len(train_loader.dataset)\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device)\n",
        "            m = m.to(device)\n",
        "            p = p.to(device)\n",
        "            # Compute prediction and loss\n",
        "            pred = model(x)\n",
        "            loss_1 = weighted_rmse_loss(pred, y, m)\n",
        "            loss_2, a, b, c = calculate_ec_loss(x,\n",
        "                                        pred,\n",
        "                                        p,                                     \n",
        "                                        depth_areas,\n",
        "                                        n_depths,\n",
        "                                        ec_threshold,\n",
        "                                        combine_days=1)\n",
        "            loss = loss_1 + elam * loss_2\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Print output\n",
        "            loss_track, current = loss.item(), batch * len(x)\n",
        "            if batch%5 == 0:\n",
        "                print(\"BatLoss = \" + \"{:.4f}\".format(loss) + \\\n",
        "                    \", Rc = \" + \"{:.4f}\".format(loss_1) + \\\n",
        "                    \", Ec = \" + \"{:.4f}\".format(loss_2) + \\\n",
        "                    f'[{current:>5d}/{size:>5d}]')\n",
        "        \n",
        "        # Early stopping\n",
        "        if early_stopping:\n",
        "            the_current_loss = val_test(model, valid_loader)\n",
        "            print('The current loss:', the_current_loss)\n",
        "            if the_current_loss > the_last_loss:\n",
        "                trigger_times += 1\n",
        "                if trigger_times >= patience:\n",
        "                    print('Early stopping!\\nStart to test process.')\n",
        "                    return model, (i+1)\n",
        "            else:\n",
        "                trigger_times = 0 ## reset trigger time to 0\n",
        "                the_last_loss = the_current_loss\n",
        "\n",
        "    return model, epochs\n",
        "        \n",
        "\n",
        "def val_test(model, data_loader):\n",
        "    # Settings\n",
        "    model.eval()\n",
        "\n",
        "    loss_total = 0\n",
        "    # make predictions on validation/test data\n",
        "    with torch.no_grad():\n",
        "        for batch, (x, y, m, p) in enumerate(data_loader):\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device)\n",
        "            m = m.to(device)\n",
        "            p = p.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model(x)\n",
        "            loss_1 = weighted_rmse_loss(pred, y, m)\n",
        "            loss_2, a, b, c = calculate_ec_loss(x,\n",
        "                                        pred,\n",
        "                                        p,                                     \n",
        "                                        depth_areas,\n",
        "                                        n_depths,\n",
        "                                        ec_threshold,\n",
        "                                        combine_days=1)\n",
        "            loss = loss_1 + elam * loss_2\n",
        "            loss_total += loss.item()\n",
        "    return loss_total / len(data_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "8jvWQQCFGaDP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create data loaders"
      ],
      "metadata": {
        "id": "yT_XdYsit0f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(x_train), \n",
        "                           torch.from_numpy(y_train), \n",
        "                           torch.from_numpy(m_train),\n",
        "                           torch.from_numpy(p_train))\n",
        "\n",
        "val_data = TensorDataset(torch.from_numpy(x_val), \n",
        "                         torch.from_numpy(y_val), \n",
        "                         torch.from_numpy(m_val),\n",
        "                         torch.from_numpy(p_val))\n",
        "\n",
        "full_data = TensorDataset(torch.from_numpy(x_f), \n",
        "                          torch.from_numpy(y_f), \n",
        "                          torch.from_numpy(m_f),\n",
        "                          torch.from_numpy(p_f))\n",
        "\n",
        "tr_loader = DataLoader(train_data, batch_sampler=batch_sample_generator(len(train_data), batch_size))\n",
        "va_loader = DataLoader(val_data, batch_sampler=batch_sample_generator(len(val_data), batch_size))\n",
        "fu_loader = DataLoader(full_data, batch_sampler=batch_sample_generator(len(full_data), batch_size))"
      ],
      "metadata": {
        "id": "JVKzySyqtz7k"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with early-stopping"
      ],
      "metadata": {
        "id": "I2wL-ZCGmq6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we train the model with early stopping. That is, if the model "
      ],
      "metadata": {
        "id": "AU051Gg0E9TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with early stopping\n",
        "## initialize model\n",
        "net = LSTMnet(input_dim = input_size, output_dim = 1, hidden_dim = state_size, n_layers = 1).to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "net, best_epoch = train(net, epochs, optimizer, tr_loader, va_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkR75nmuKeHA",
        "outputId": "4c32e64a-95fe-4206-9975-51eab49f050c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is epoch 1\n",
            "BatLoss = 11.2963, Rc = 10.1567, Ec = 227.9222[    0/  950]\n",
            "BatLoss = 11.5455, Rc = 10.4007, Ec = 228.9714[  500/  950]\n",
            "The current loss: 11.245773403734882\n",
            "This is epoch 2\n",
            "BatLoss = 10.8612, Rc = 9.7458, Ec = 223.0675[    0/  950]\n",
            "BatLoss = 10.9723, Rc = 9.8603, Ec = 222.3923[  500/  950]\n",
            "The current loss: 10.528699237325462\n",
            "This is epoch 3\n",
            "BatLoss = 10.1454, Rc = 9.0557, Ec = 217.9277[    0/  950]\n",
            "BatLoss = 10.1994, Rc = 9.1269, Ec = 214.4844[  500/  950]\n",
            "The current loss: 9.74712720419046\n",
            "This is epoch 4\n",
            "BatLoss = 9.3406, Rc = 8.2826, Ec = 211.6186[    0/  950]\n",
            "BatLoss = 9.4235, Rc = 8.3880, Ec = 207.0916[  500/  950]\n",
            "The current loss: 8.893480282794403\n",
            "This is epoch 5\n",
            "BatLoss = 8.4845, Rc = 7.4803, Ec = 200.8374[    0/  950]\n",
            "BatLoss = 8.5243, Rc = 7.5584, Ec = 193.1699[  500/  950]\n",
            "The current loss: 8.082442582936993\n",
            "This is epoch 6\n",
            "BatLoss = 7.6781, Rc = 6.7401, Ec = 187.5931[    0/  950]\n",
            "BatLoss = 7.7553, Rc = 6.8617, Ec = 178.7142[  500/  950]\n",
            "The current loss: 7.386388451421863\n",
            "This is epoch 7\n",
            "BatLoss = 6.9744, Rc = 6.1033, Ec = 174.2290[    0/  950]\n",
            "BatLoss = 7.0823, Rc = 6.2340, Ec = 169.6683[  500/  950]\n",
            "The current loss: 6.767220796641371\n",
            "This is epoch 8\n",
            "BatLoss = 6.4444, Rc = 5.5944, Ec = 170.0023[    0/  950]\n",
            "BatLoss = 6.4689, Rc = 5.6727, Ec = 159.2333[  500/  950]\n",
            "The current loss: 6.217759045872529\n",
            "This is epoch 9\n",
            "BatLoss = 5.9429, Rc = 5.1297, Ec = 162.6472[    0/  950]\n",
            "BatLoss = 5.9347, Rc = 5.1846, Ec = 150.0188[  500/  950]\n",
            "The current loss: 5.740959134104968\n",
            "This is epoch 10\n",
            "BatLoss = 5.4046, Rc = 4.6466, Ec = 151.6058[    0/  950]\n",
            "BatLoss = 5.4561, Rc = 4.7441, Ec = 142.4043[  500/  950]\n",
            "The current loss: 5.311490533671368\n",
            "This is epoch 11\n",
            "BatLoss = 5.0968, Rc = 4.3661, Ec = 146.1283[    0/  950]\n",
            "BatLoss = 5.0036, Rc = 4.3396, Ec = 132.8007[  500/  950]\n",
            "The current loss: 4.9575749948765395\n",
            "This is epoch 12\n",
            "BatLoss = 4.7437, Rc = 4.0556, Ec = 137.6214[    0/  950]\n",
            "BatLoss = 4.6434, Rc = 4.0098, Ec = 126.7325[  500/  950]\n",
            "The current loss: 4.663288576909965\n",
            "This is epoch 13\n",
            "BatLoss = 4.4748, Rc = 3.8199, Ec = 130.9836[    0/  950]\n",
            "BatLoss = 4.3333, Rc = 3.7284, Ec = 120.9780[  500/  950]\n",
            "The current loss: 4.374281189323092\n",
            "This is epoch 14\n",
            "BatLoss = 4.1475, Rc = 3.5452, Ec = 120.4709[    0/  950]\n",
            "BatLoss = 4.0527, Rc = 3.5030, Ec = 109.9522[  500/  950]\n",
            "The current loss: 4.168300298618559\n",
            "This is epoch 15\n",
            "BatLoss = 3.9344, Rc = 3.3785, Ec = 111.1713[    0/  950]\n",
            "BatLoss = 3.8777, Rc = 3.3525, Ec = 105.0425[  500/  950]\n",
            "The current loss: 3.9988298401832787\n",
            "This is epoch 16\n",
            "BatLoss = 3.7779, Rc = 3.2559, Ec = 104.4037[    0/  950]\n",
            "BatLoss = 3.7412, Rc = 3.2372, Ec = 100.8023[  500/  950]\n",
            "The current loss: 3.8593516081043004\n",
            "This is epoch 17\n",
            "BatLoss = 3.6647, Rc = 3.1653, Ec = 99.8799[    0/  950]\n",
            "BatLoss = 3.5924, Rc = 3.1189, Ec = 94.6953[  500/  950]\n",
            "The current loss: 3.735524866306393\n",
            "This is epoch 18\n",
            "BatLoss = 3.4979, Rc = 3.0106, Ec = 97.4566[    0/  950]\n",
            "BatLoss = 3.4299, Rc = 2.9821, Ec = 89.5589[  500/  950]\n",
            "The current loss: 3.570581022191334\n",
            "This is epoch 19\n",
            "BatLoss = 3.3316, Rc = 2.8698, Ec = 92.3596[    0/  950]\n",
            "BatLoss = 3.2614, Rc = 2.8319, Ec = 85.8820[  500/  950]\n",
            "The current loss: 3.400629578381269\n",
            "This is epoch 20\n",
            "BatLoss = 3.1583, Rc = 2.7088, Ec = 89.9004[    0/  950]\n",
            "BatLoss = 3.0567, Rc = 2.6471, Ec = 81.9102[  500/  950]\n",
            "The current loss: 3.2468571831866213\n",
            "This is epoch 21\n",
            "BatLoss = 3.0610, Rc = 2.6214, Ec = 87.9313[    0/  950]\n",
            "BatLoss = 2.8974, Rc = 2.5055, Ec = 78.3708[  500/  950]\n",
            "The current loss: 3.097995840801812\n",
            "This is epoch 22\n",
            "BatLoss = 2.8738, Rc = 2.4631, Ec = 82.1308[    0/  950]\n",
            "BatLoss = 2.7657, Rc = 2.3890, Ec = 75.3266[  500/  950]\n",
            "The current loss: 2.9652681449229052\n",
            "This is epoch 23\n",
            "BatLoss = 2.7265, Rc = 2.3498, Ec = 75.3360[    0/  950]\n",
            "BatLoss = 2.6294, Rc = 2.2781, Ec = 70.2557[  500/  950]\n",
            "The current loss: 2.8444306597115743\n",
            "This is epoch 24\n",
            "BatLoss = 2.6145, Rc = 2.2543, Ec = 72.0479[    0/  950]\n",
            "BatLoss = 2.4667, Rc = 2.1399, Ec = 65.3544[  500/  950]\n",
            "The current loss: 2.684624276811861\n",
            "This is epoch 25\n",
            "BatLoss = 2.4385, Rc = 2.0946, Ec = 68.7902[    0/  950]\n",
            "BatLoss = 2.3268, Rc = 2.0182, Ec = 61.7195[  500/  950]\n",
            "The current loss: 2.551746350858246\n",
            "This is epoch 26\n",
            "BatLoss = 2.2574, Rc = 1.9459, Ec = 62.2870[    0/  950]\n",
            "BatLoss = 2.1741, Rc = 1.8820, Ec = 58.4235[  500/  950]\n",
            "The current loss: 2.4511904816452454\n",
            "This is epoch 27\n",
            "BatLoss = 2.1640, Rc = 1.8584, Ec = 61.1203[    0/  950]\n",
            "BatLoss = 2.0889, Rc = 1.8172, Ec = 54.3348[  500/  950]\n",
            "The current loss: 2.344816609491696\n",
            "This is epoch 28\n",
            "BatLoss = 2.0750, Rc = 1.7867, Ec = 57.6582[    0/  950]\n",
            "BatLoss = 1.9476, Rc = 1.6834, Ec = 52.8480[  500/  950]\n",
            "The current loss: 2.2477288979820105\n",
            "This is epoch 29\n",
            "BatLoss = 2.0025, Rc = 1.7151, Ec = 57.4805[    0/  950]\n",
            "BatLoss = 1.8256, Rc = 1.5750, Ec = 50.1098[  500/  950]\n",
            "The current loss: 2.125712209871129\n",
            "This is epoch 30\n",
            "BatLoss = 1.7996, Rc = 1.5524, Ec = 49.4456[    0/  950]\n",
            "BatLoss = 1.7385, Rc = 1.5000, Ec = 47.7143[  500/  950]\n",
            "The current loss: 2.084791346735584\n",
            "This is epoch 31\n",
            "BatLoss = 1.7507, Rc = 1.5130, Ec = 47.5504[    0/  950]\n",
            "BatLoss = 1.6889, Rc = 1.4610, Ec = 45.5723[  500/  950]\n",
            "The current loss: 1.975766515715924\n",
            "This is epoch 32\n",
            "BatLoss = 1.6149, Rc = 1.3894, Ec = 45.1002[    0/  950]\n",
            "BatLoss = 1.5810, Rc = 1.3684, Ec = 42.5255[  500/  950]\n",
            "The current loss: 1.8887189413537235\n",
            "This is epoch 33\n",
            "BatLoss = 1.5291, Rc = 1.3253, Ec = 40.7675[    0/  950]\n",
            "BatLoss = 1.4837, Rc = 1.2958, Ec = 37.5718[  500/  950]\n",
            "The current loss: 1.8463550111633666\n",
            "This is epoch 34\n",
            "BatLoss = 1.5296, Rc = 1.3238, Ec = 41.1541[    0/  950]\n",
            "BatLoss = 1.3797, Rc = 1.2072, Ec = 34.4904[  500/  950]\n",
            "The current loss: 1.7830281320293722\n",
            "This is epoch 35\n",
            "BatLoss = 1.4411, Rc = 1.2484, Ec = 38.5315[    0/  950]\n",
            "BatLoss = 1.3384, Rc = 1.1799, Ec = 31.6868[  500/  950]\n",
            "The current loss: 1.7653182377523238\n",
            "This is epoch 36\n",
            "BatLoss = 1.4242, Rc = 1.2411, Ec = 36.6094[    0/  950]\n",
            "BatLoss = 1.3324, Rc = 1.1869, Ec = 29.0993[  500/  950]\n",
            "The current loss: 1.7837064567388985\n",
            "This is epoch 37\n",
            "BatLoss = 1.4616, Rc = 1.2770, Ec = 36.9130[    0/  950]\n",
            "BatLoss = 1.3617, Rc = 1.2218, Ec = 27.9892[  500/  950]\n",
            "The current loss: 1.8307569877791028\n",
            "This is epoch 38\n",
            "BatLoss = 1.5434, Rc = 1.3534, Ec = 37.9887[    0/  950]\n",
            "BatLoss = 1.3083, Rc = 1.1852, Ec = 24.6255[  500/  950]\n",
            "The current loss: 1.761986275256196\n",
            "This is epoch 39\n",
            "BatLoss = 1.4724, Rc = 1.3094, Ec = 32.5914[    0/  950]\n",
            "BatLoss = 1.6153, Rc = 1.4704, Ec = 28.9928[  500/  950]\n",
            "The current loss: 1.749027714602958\n",
            "This is epoch 40\n",
            "BatLoss = 1.4980, Rc = 1.3458, Ec = 30.4287[    0/  950]\n",
            "BatLoss = 1.3560, Rc = 1.2267, Ec = 25.8665[  500/  950]\n",
            "The current loss: 1.5088356016604085\n",
            "This is epoch 41\n",
            "BatLoss = 1.1854, Rc = 1.0497, Ec = 27.1291[    0/  950]\n",
            "BatLoss = 1.0444, Rc = 0.9316, Ec = 22.5710[  500/  950]\n",
            "The current loss: 1.530316119149522\n",
            "This is epoch 42\n",
            "BatLoss = 1.2170, Rc = 1.0856, Ec = 26.2864[    0/  950]\n",
            "BatLoss = 1.0091, Rc = 0.9080, Ec = 20.2106[  500/  950]\n",
            "The current loss: 1.4775579543484316\n",
            "This is epoch 43\n",
            "BatLoss = 1.1801, Rc = 1.0675, Ec = 22.5251[    0/  950]\n",
            "BatLoss = 1.0241, Rc = 0.9190, Ec = 21.0190[  500/  950]\n",
            "The current loss: 1.4349767387376549\n",
            "This is epoch 44\n",
            "BatLoss = 1.1440, Rc = 1.0434, Ec = 20.1157[    0/  950]\n",
            "BatLoss = 1.0261, Rc = 0.9282, Ec = 19.5862[  500/  950]\n",
            "The current loss: 1.4385486023442724\n",
            "This is epoch 45\n",
            "BatLoss = 1.1399, Rc = 1.0473, Ec = 18.5212[    0/  950]\n",
            "BatLoss = 1.0281, Rc = 0.9454, Ec = 16.5541[  500/  950]\n",
            "The current loss: 1.4209338550239548\n",
            "This is epoch 46\n",
            "BatLoss = 1.1314, Rc = 1.0365, Ec = 18.9778[    0/  950]\n",
            "BatLoss = 0.9944, Rc = 0.9243, Ec = 14.0242[  500/  950]\n",
            "The current loss: 1.3848770661351495\n",
            "This is epoch 47\n",
            "BatLoss = 1.1066, Rc = 1.0088, Ec = 19.5485[    0/  950]\n",
            "BatLoss = 0.9671, Rc = 0.9008, Ec = 13.2657[  500/  950]\n",
            "The current loss: 1.3596082017450433\n",
            "This is epoch 48\n",
            "BatLoss = 1.0706, Rc = 0.9735, Ec = 19.4240[    0/  950]\n",
            "BatLoss = 0.9450, Rc = 0.8800, Ec = 13.0184[  500/  950]\n",
            "The current loss: 1.3470419153491258\n",
            "This is epoch 49\n",
            "BatLoss = 1.0440, Rc = 0.9524, Ec = 18.3229[    0/  950]\n",
            "BatLoss = 0.9321, Rc = 0.8750, Ec = 11.4267[  500/  950]\n",
            "The current loss: 1.3390299894317788\n",
            "This is epoch 50\n",
            "BatLoss = 1.0321, Rc = 0.9467, Ec = 17.0803[    0/  950]\n",
            "BatLoss = 0.9188, Rc = 0.8651, Ec = 10.7343[  500/  950]\n",
            "The current loss: 1.3260521991661203\n",
            "This is epoch 51\n",
            "BatLoss = 1.0228, Rc = 0.9403, Ec = 16.5060[    0/  950]\n",
            "BatLoss = 0.9025, Rc = 0.8515, Ec = 10.2000[  500/  950]\n",
            "The current loss: 1.3111592174134488\n",
            "This is epoch 52\n",
            "BatLoss = 1.0107, Rc = 0.9350, Ec = 15.1279[    0/  950]\n",
            "BatLoss = 0.8877, Rc = 0.8359, Ec = 10.3650[  500/  950]\n",
            "The current loss: 1.3068848626123004\n",
            "This is epoch 53\n",
            "BatLoss = 1.0022, Rc = 0.9326, Ec = 13.9041[    0/  950]\n",
            "BatLoss = 0.8767, Rc = 0.8252, Ec = 10.3050[  500/  950]\n",
            "The current loss: 1.3149847308751732\n",
            "This is epoch 54\n",
            "BatLoss = 1.0021, Rc = 0.9379, Ec = 12.8468[    0/  950]\n",
            "BatLoss = 0.8721, Rc = 0.8200, Ec = 10.4273[  500/  950]\n",
            "The current loss: 1.3392015828547132\n",
            "This is epoch 55\n",
            "BatLoss = 1.0173, Rc = 0.9549, Ec = 12.4862[    0/  950]\n",
            "BatLoss = 0.8656, Rc = 0.8135, Ec = 10.4217[  500/  950]\n",
            "The current loss: 1.34833063319302\n",
            "Early stopping!\n",
            "Start to test process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain with best parameters\n",
        "During the training, the training segment and validation segment were used separately to identify the best epochs. Once a best set of paramemters are identified based on the loss evaluated using the validation set, the model should be retrained using the combined training and validation set to have the best estimation efficiency. "
      ],
      "metadata": {
        "id": "fbR8wABNmuE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional: retrain the model using the best #epochs over train+val\n",
        "## initialize model\n",
        "net = LSTMnet(input_dim = input_size, output_dim = 1, hidden_dim = state_size, n_layers = 1).to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "net, _ = train(net, best_epoch, optimizer, fu_loader, fu_loader, early_stopping=False)\n",
        "## save the model"
      ],
      "metadata": {
        "id": "R3u8fI0SMCJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfe1b70-e2e8-4c1a-b159-a14e37593008"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is epoch 1\n",
            "BatLoss = 11.5584, Rc = 10.4112, Ec = 229.4359[    0/ 1900]\n",
            "BatLoss = 11.7983, Rc = 10.6462, Ec = 230.4039[  500/ 1900]\n",
            "BatLoss = 10.3465, Rc = 9.2898, Ec = 211.3329[ 1000/ 1900]\n",
            "BatLoss = 11.1775, Rc = 10.0589, Ec = 223.7161[ 1500/ 1900]\n",
            "This is epoch 2\n",
            "BatLoss = 10.4985, Rc = 9.4099, Ec = 217.7088[    0/ 1900]\n",
            "BatLoss = 10.5561, Rc = 9.4723, Ec = 216.7681[  500/ 1900]\n",
            "BatLoss = 8.9155, Rc = 7.9426, Ec = 194.5915[ 1000/ 1900]\n",
            "BatLoss = 9.3859, Rc = 8.3762, Ec = 201.9262[ 1500/ 1900]\n",
            "This is epoch 3\n",
            "BatLoss = 8.6666, Rc = 7.6736, Ec = 198.5993[    0/ 1900]\n",
            "BatLoss = 8.7686, Rc = 7.7891, Ec = 195.9043[  500/ 1900]\n",
            "BatLoss = 7.4280, Rc = 6.5462, Ec = 176.3684[ 1000/ 1900]\n",
            "BatLoss = 7.8757, Rc = 6.9647, Ec = 182.1928[ 1500/ 1900]\n",
            "This is epoch 4\n",
            "BatLoss = 7.3722, Rc = 6.4509, Ec = 184.2553[    0/ 1900]\n",
            "BatLoss = 7.5063, Rc = 6.6134, Ec = 178.5730[  500/ 1900]\n",
            "BatLoss = 6.3758, Rc = 5.5631, Ec = 162.5422[ 1000/ 1900]\n",
            "BatLoss = 6.7132, Rc = 5.8753, Ec = 167.5727[ 1500/ 1900]\n",
            "This is epoch 5\n",
            "BatLoss = 6.3173, Rc = 5.4715, Ec = 169.1441[    0/ 1900]\n",
            "BatLoss = 6.3759, Rc = 5.5776, Ec = 159.6601[  500/ 1900]\n",
            "BatLoss = 5.5462, Rc = 4.7996, Ec = 149.3107[ 1000/ 1900]\n",
            "BatLoss = 5.8127, Rc = 4.9999, Ec = 162.5716[ 1500/ 1900]\n",
            "This is epoch 6\n",
            "BatLoss = 5.4178, Rc = 4.6329, Ec = 156.9893[    0/ 1900]\n",
            "BatLoss = 5.4683, Rc = 4.7245, Ec = 148.7647[  500/ 1900]\n",
            "BatLoss = 4.8799, Rc = 4.1828, Ec = 139.4143[ 1000/ 1900]\n",
            "BatLoss = 5.0865, Rc = 4.3293, Ec = 151.4381[ 1500/ 1900]\n",
            "This is epoch 7\n",
            "BatLoss = 4.7055, Rc = 3.9771, Ec = 145.6851[    0/ 1900]\n",
            "BatLoss = 4.6812, Rc = 4.0387, Ec = 128.5092[  500/ 1900]\n",
            "BatLoss = 4.4265, Rc = 3.8434, Ec = 116.6104[ 1000/ 1900]\n",
            "BatLoss = 4.4845, Rc = 3.8568, Ec = 125.5318[ 1500/ 1900]\n",
            "This is epoch 8\n",
            "BatLoss = 4.2473, Rc = 3.6180, Ec = 125.8562[    0/ 1900]\n",
            "BatLoss = 4.1735, Rc = 3.5931, Ec = 116.0891[  500/ 1900]\n",
            "BatLoss = 4.1963, Rc = 3.6907, Ec = 101.1215[ 1000/ 1900]\n",
            "BatLoss = 4.0741, Rc = 3.5001, Ec = 114.8099[ 1500/ 1900]\n",
            "This is epoch 9\n",
            "BatLoss = 3.9174, Rc = 3.3601, Ec = 111.4598[    0/ 1900]\n",
            "BatLoss = 3.8415, Rc = 3.3257, Ec = 103.1501[  500/ 1900]\n",
            "BatLoss = 4.1168, Rc = 3.6531, Ec = 92.7243[ 1000/ 1900]\n",
            "BatLoss = 3.8197, Rc = 3.3033, Ec = 103.2814[ 1500/ 1900]\n",
            "This is epoch 10\n",
            "BatLoss = 3.6688, Rc = 3.1594, Ec = 101.8844[    0/ 1900]\n",
            "BatLoss = 3.5682, Rc = 3.0985, Ec = 93.9370[  500/ 1900]\n",
            "BatLoss = 3.6437, Rc = 3.2351, Ec = 81.7165[ 1000/ 1900]\n",
            "BatLoss = 3.5193, Rc = 3.0354, Ec = 96.7847[ 1500/ 1900]\n",
            "This is epoch 11\n",
            "BatLoss = 3.3231, Rc = 2.8563, Ec = 93.3642[    0/ 1900]\n",
            "BatLoss = 3.2522, Rc = 2.8209, Ec = 86.2588[  500/ 1900]\n",
            "BatLoss = 3.3707, Rc = 2.9997, Ec = 74.1844[ 1000/ 1900]\n",
            "BatLoss = 3.2198, Rc = 2.7914, Ec = 85.6923[ 1500/ 1900]\n",
            "This is epoch 12\n",
            "BatLoss = 3.0382, Rc = 2.6170, Ec = 84.2487[    0/ 1900]\n",
            "BatLoss = 2.9586, Rc = 2.5691, Ec = 77.9005[  500/ 1900]\n",
            "BatLoss = 3.0965, Rc = 2.7657, Ec = 66.1699[ 1000/ 1900]\n",
            "BatLoss = 2.9465, Rc = 2.5734, Ec = 74.6265[ 1500/ 1900]\n",
            "This is epoch 13\n",
            "BatLoss = 2.7346, Rc = 2.3824, Ec = 70.4388[    0/ 1900]\n",
            "BatLoss = 2.6866, Rc = 2.3576, Ec = 65.8031[  500/ 1900]\n",
            "BatLoss = 2.8344, Rc = 2.5481, Ec = 57.2611[ 1000/ 1900]\n",
            "BatLoss = 2.6981, Rc = 2.3615, Ec = 67.3388[ 1500/ 1900]\n",
            "This is epoch 14\n",
            "BatLoss = 2.4469, Rc = 2.1367, Ec = 62.0404[    0/ 1900]\n",
            "BatLoss = 2.4228, Rc = 2.1371, Ec = 57.1443[  500/ 1900]\n",
            "BatLoss = 2.6204, Rc = 2.3722, Ec = 49.6359[ 1000/ 1900]\n",
            "BatLoss = 2.4675, Rc = 2.1641, Ec = 60.6908[ 1500/ 1900]\n",
            "This is epoch 15\n",
            "BatLoss = 2.2322, Rc = 1.9533, Ec = 55.7743[    0/ 1900]\n",
            "BatLoss = 2.1793, Rc = 1.9288, Ec = 50.1061[  500/ 1900]\n",
            "BatLoss = 2.4702, Rc = 2.2565, Ec = 42.7477[ 1000/ 1900]\n",
            "BatLoss = 2.2656, Rc = 1.9899, Ec = 55.1502[ 1500/ 1900]\n",
            "This is epoch 16\n",
            "BatLoss = 2.0447, Rc = 1.7895, Ec = 51.0428[    0/ 1900]\n",
            "BatLoss = 1.9658, Rc = 1.7459, Ec = 43.9696[  500/ 1900]\n",
            "BatLoss = 2.3527, Rc = 2.1678, Ec = 36.9891[ 1000/ 1900]\n",
            "BatLoss = 2.0954, Rc = 1.8348, Ec = 52.1360[ 1500/ 1900]\n",
            "This is epoch 17\n",
            "BatLoss = 1.9079, Rc = 1.6695, Ec = 47.6931[    0/ 1900]\n",
            "BatLoss = 1.8017, Rc = 1.6049, Ec = 39.3613[  500/ 1900]\n",
            "BatLoss = 2.2352, Rc = 2.0788, Ec = 31.2918[ 1000/ 1900]\n",
            "BatLoss = 1.9632, Rc = 1.7220, Ec = 48.2369[ 1500/ 1900]\n",
            "This is epoch 18\n",
            "BatLoss = 1.7767, Rc = 1.5553, Ec = 44.2760[    0/ 1900]\n",
            "BatLoss = 1.6772, Rc = 1.4976, Ec = 35.9164[  500/ 1900]\n",
            "BatLoss = 2.1048, Rc = 1.9752, Ec = 25.9304[ 1000/ 1900]\n",
            "BatLoss = 1.8271, Rc = 1.6169, Ec = 42.0374[ 1500/ 1900]\n",
            "This is epoch 19\n",
            "BatLoss = 1.6409, Rc = 1.4415, Ec = 39.8760[    0/ 1900]\n",
            "BatLoss = 1.5713, Rc = 1.4121, Ec = 31.8522[  500/ 1900]\n",
            "BatLoss = 2.0386, Rc = 1.9311, Ec = 21.5121[ 1000/ 1900]\n",
            "BatLoss = 1.6817, Rc = 1.4995, Ec = 36.4359[ 1500/ 1900]\n",
            "This is epoch 20\n",
            "BatLoss = 1.5228, Rc = 1.3384, Ec = 36.8797[    0/ 1900]\n",
            "BatLoss = 1.4825, Rc = 1.3390, Ec = 28.6943[  500/ 1900]\n",
            "BatLoss = 1.9728, Rc = 1.8874, Ec = 17.0814[ 1000/ 1900]\n",
            "BatLoss = 1.5636, Rc = 1.3998, Ec = 32.7562[ 1500/ 1900]\n",
            "This is epoch 21\n",
            "BatLoss = 1.4163, Rc = 1.2523, Ec = 32.7893[    0/ 1900]\n",
            "BatLoss = 1.4142, Rc = 1.2821, Ec = 26.4234[  500/ 1900]\n",
            "BatLoss = 1.9038, Rc = 1.8380, Ec = 13.1545[ 1000/ 1900]\n",
            "BatLoss = 1.4687, Rc = 1.3249, Ec = 28.7595[ 1500/ 1900]\n",
            "This is epoch 22\n",
            "BatLoss = 1.3232, Rc = 1.1793, Ec = 28.7801[    0/ 1900]\n",
            "BatLoss = 1.3511, Rc = 1.2354, Ec = 23.1398[  500/ 1900]\n",
            "BatLoss = 1.8602, Rc = 1.8066, Ec = 10.7314[ 1000/ 1900]\n",
            "BatLoss = 1.3728, Rc = 1.2423, Ec = 26.0885[ 1500/ 1900]\n",
            "This is epoch 23\n",
            "BatLoss = 1.2509, Rc = 1.1215, Ec = 25.8659[    0/ 1900]\n",
            "BatLoss = 1.2918, Rc = 1.1861, Ec = 21.1509[  500/ 1900]\n",
            "BatLoss = 1.8131, Rc = 1.7731, Ec = 7.9952[ 1000/ 1900]\n",
            "BatLoss = 1.2997, Rc = 1.1886, Ec = 22.2193[ 1500/ 1900]\n",
            "This is epoch 24\n",
            "BatLoss = 1.1960, Rc = 1.0774, Ec = 23.7215[    0/ 1900]\n",
            "BatLoss = 1.2284, Rc = 1.1314, Ec = 19.4009[  500/ 1900]\n",
            "BatLoss = 1.7815, Rc = 1.7473, Ec = 6.8332[ 1000/ 1900]\n",
            "BatLoss = 1.2466, Rc = 1.1422, Ec = 20.8886[ 1500/ 1900]\n",
            "This is epoch 25\n",
            "BatLoss = 1.1538, Rc = 1.0453, Ec = 21.6887[    0/ 1900]\n",
            "BatLoss = 1.1740, Rc = 1.0845, Ec = 17.9171[  500/ 1900]\n",
            "BatLoss = 1.7595, Rc = 1.7354, Ec = 4.8125[ 1000/ 1900]\n",
            "BatLoss = 1.2049, Rc = 1.1143, Ec = 18.1332[ 1500/ 1900]\n",
            "This is epoch 26\n",
            "BatLoss = 1.1264, Rc = 1.0264, Ec = 19.9997[    0/ 1900]\n",
            "BatLoss = 1.1362, Rc = 1.0547, Ec = 16.3134[  500/ 1900]\n",
            "BatLoss = 1.7495, Rc = 1.7246, Ec = 4.9697[ 1000/ 1900]\n",
            "BatLoss = 1.1628, Rc = 1.0754, Ec = 17.4838[ 1500/ 1900]\n",
            "This is epoch 27\n",
            "BatLoss = 1.0969, Rc = 1.0082, Ec = 17.7304[    0/ 1900]\n",
            "BatLoss = 1.1015, Rc = 1.0269, Ec = 14.9194[  500/ 1900]\n",
            "BatLoss = 1.7370, Rc = 1.7199, Ec = 3.4219[ 1000/ 1900]\n",
            "BatLoss = 1.1415, Rc = 1.0619, Ec = 15.9317[ 1500/ 1900]\n",
            "This is epoch 28\n",
            "BatLoss = 1.0685, Rc = 0.9858, Ec = 16.5485[    0/ 1900]\n",
            "BatLoss = 1.0807, Rc = 1.0083, Ec = 14.4825[  500/ 1900]\n",
            "BatLoss = 1.7147, Rc = 1.6994, Ec = 3.0563[ 1000/ 1900]\n",
            "BatLoss = 1.1108, Rc = 1.0371, Ec = 14.7290[ 1500/ 1900]\n",
            "This is epoch 29\n",
            "BatLoss = 1.0434, Rc = 0.9694, Ec = 14.8027[    0/ 1900]\n",
            "BatLoss = 1.0421, Rc = 0.9751, Ec = 13.4015[  500/ 1900]\n",
            "BatLoss = 1.7029, Rc = 1.6930, Ec = 1.9881[ 1000/ 1900]\n",
            "BatLoss = 1.0970, Rc = 1.0273, Ec = 13.9316[ 1500/ 1900]\n",
            "This is epoch 30\n",
            "BatLoss = 1.0267, Rc = 0.9580, Ec = 13.7517[    0/ 1900]\n",
            "BatLoss = 1.0275, Rc = 0.9636, Ec = 12.7726[  500/ 1900]\n",
            "BatLoss = 1.6951, Rc = 1.6837, Ec = 2.2886[ 1000/ 1900]\n",
            "BatLoss = 1.0815, Rc = 1.0149, Ec = 13.3156[ 1500/ 1900]\n",
            "This is epoch 31\n",
            "BatLoss = 1.0191, Rc = 0.9548, Ec = 12.8584[    0/ 1900]\n",
            "BatLoss = 1.0090, Rc = 0.9470, Ec = 12.4088[  500/ 1900]\n",
            "BatLoss = 1.6920, Rc = 1.6841, Ec = 1.5808[ 1000/ 1900]\n",
            "BatLoss = 1.0685, Rc = 1.0064, Ec = 12.4217[ 1500/ 1900]\n",
            "This is epoch 32\n",
            "BatLoss = 1.0097, Rc = 0.9464, Ec = 12.6613[    0/ 1900]\n",
            "BatLoss = 0.9896, Rc = 0.9339, Ec = 11.1440[  500/ 1900]\n",
            "BatLoss = 1.6818, Rc = 1.6715, Ec = 2.0538[ 1000/ 1900]\n",
            "BatLoss = 1.0550, Rc = 0.9961, Ec = 11.7833[ 1500/ 1900]\n",
            "This is epoch 33\n",
            "BatLoss = 0.9957, Rc = 0.9394, Ec = 11.2676[    0/ 1900]\n",
            "BatLoss = 0.9760, Rc = 0.9217, Ec = 10.8524[  500/ 1900]\n",
            "BatLoss = 1.6775, Rc = 1.6707, Ec = 1.3610[ 1000/ 1900]\n",
            "BatLoss = 1.0489, Rc = 0.9924, Ec = 11.2860[ 1500/ 1900]\n",
            "This is epoch 34\n",
            "BatLoss = 0.9878, Rc = 0.9312, Ec = 11.3171[    0/ 1900]\n",
            "BatLoss = 0.9642, Rc = 0.9126, Ec = 10.3263[  500/ 1900]\n",
            "BatLoss = 1.6670, Rc = 1.6578, Ec = 1.8311[ 1000/ 1900]\n",
            "BatLoss = 1.0400, Rc = 0.9853, Ec = 10.9303[ 1500/ 1900]\n",
            "This is epoch 35\n",
            "BatLoss = 0.9811, Rc = 0.9286, Ec = 10.5083[    0/ 1900]\n",
            "BatLoss = 0.9519, Rc = 0.9012, Ec = 10.1312[  500/ 1900]\n",
            "BatLoss = 1.6574, Rc = 1.6500, Ec = 1.4824[ 1000/ 1900]\n",
            "BatLoss = 1.0384, Rc = 0.9853, Ec = 10.6104[ 1500/ 1900]\n",
            "This is epoch 36\n",
            "BatLoss = 0.9872, Rc = 0.9361, Ec = 10.2200[    0/ 1900]\n",
            "BatLoss = 0.9552, Rc = 0.9053, Ec = 9.9960[  500/ 1900]\n",
            "BatLoss = 1.6532, Rc = 1.6449, Ec = 1.6642[ 1000/ 1900]\n",
            "BatLoss = 1.0342, Rc = 0.9804, Ec = 10.7625[ 1500/ 1900]\n",
            "This is epoch 37\n",
            "BatLoss = 0.9928, Rc = 0.9443, Ec = 9.7005[    0/ 1900]\n",
            "BatLoss = 0.9658, Rc = 0.9129, Ec = 10.5847[  500/ 1900]\n",
            "BatLoss = 1.6531, Rc = 1.6456, Ec = 1.5017[ 1000/ 1900]\n",
            "BatLoss = 1.0440, Rc = 0.9885, Ec = 11.0864[ 1500/ 1900]\n",
            "This is epoch 38\n",
            "BatLoss = 0.9648, Rc = 0.9209, Ec = 8.7859[    0/ 1900]\n",
            "BatLoss = 0.9301, Rc = 0.8826, Ec = 9.4950[  500/ 1900]\n",
            "BatLoss = 1.6326, Rc = 1.6279, Ec = 0.9548[ 1000/ 1900]\n",
            "BatLoss = 1.0303, Rc = 0.9811, Ec = 9.8346[ 1500/ 1900]\n",
            "This is epoch 39\n",
            "BatLoss = 0.9415, Rc = 0.9014, Ec = 8.0204[    0/ 1900]\n",
            "BatLoss = 0.9155, Rc = 0.8713, Ec = 8.8222[  500/ 1900]\n",
            "BatLoss = 1.6306, Rc = 1.6271, Ec = 0.7149[ 1000/ 1900]\n",
            "BatLoss = 1.0095, Rc = 0.9658, Ec = 8.7547[ 1500/ 1900]\n",
            "This is epoch 40\n",
            "BatLoss = 0.9350, Rc = 0.8960, Ec = 7.7881[    0/ 1900]\n",
            "BatLoss = 0.9151, Rc = 0.8748, Ec = 8.0451[  500/ 1900]\n",
            "BatLoss = 1.6330, Rc = 1.6286, Ec = 0.8924[ 1000/ 1900]\n",
            "BatLoss = 0.9984, Rc = 0.9577, Ec = 8.1529[ 1500/ 1900]\n",
            "This is epoch 41\n",
            "BatLoss = 0.9327, Rc = 0.8933, Ec = 7.8847[    0/ 1900]\n",
            "BatLoss = 0.9069, Rc = 0.8686, Ec = 7.6654[  500/ 1900]\n",
            "BatLoss = 1.6312, Rc = 1.6269, Ec = 0.8671[ 1000/ 1900]\n",
            "BatLoss = 0.9938, Rc = 0.9538, Ec = 8.0019[ 1500/ 1900]\n",
            "This is epoch 42\n",
            "BatLoss = 0.9315, Rc = 0.8909, Ec = 8.1230[    0/ 1900]\n",
            "BatLoss = 0.9013, Rc = 0.8638, Ec = 7.5058[  500/ 1900]\n",
            "BatLoss = 1.6274, Rc = 1.6230, Ec = 0.8818[ 1000/ 1900]\n",
            "BatLoss = 0.9882, Rc = 0.9496, Ec = 7.7252[ 1500/ 1900]\n",
            "This is epoch 43\n",
            "BatLoss = 0.9235, Rc = 0.8852, Ec = 7.6638[    0/ 1900]\n",
            "BatLoss = 0.8967, Rc = 0.8601, Ec = 7.3205[  500/ 1900]\n",
            "BatLoss = 1.6235, Rc = 1.6207, Ec = 0.5600[ 1000/ 1900]\n",
            "BatLoss = 0.9865, Rc = 0.9472, Ec = 7.8458[ 1500/ 1900]\n",
            "This is epoch 44\n",
            "BatLoss = 0.9171, Rc = 0.8799, Ec = 7.4353[    0/ 1900]\n",
            "BatLoss = 0.8967, Rc = 0.8616, Ec = 7.0236[  500/ 1900]\n",
            "BatLoss = 1.6186, Rc = 1.6171, Ec = 0.3084[ 1000/ 1900]\n",
            "BatLoss = 0.9779, Rc = 0.9414, Ec = 7.3028[ 1500/ 1900]\n",
            "This is epoch 45\n",
            "BatLoss = 0.9089, Rc = 0.8733, Ec = 7.1246[    0/ 1900]\n",
            "BatLoss = 0.8786, Rc = 0.8466, Ec = 6.3916[  500/ 1900]\n",
            "BatLoss = 1.6198, Rc = 1.6178, Ec = 0.4026[ 1000/ 1900]\n",
            "BatLoss = 0.9753, Rc = 0.9397, Ec = 7.1177[ 1500/ 1900]\n",
            "This is epoch 46\n",
            "BatLoss = 0.9016, Rc = 0.8643, Ec = 7.4701[    0/ 1900]\n",
            "BatLoss = 0.8747, Rc = 0.8430, Ec = 6.3399[  500/ 1900]\n",
            "BatLoss = 1.6173, Rc = 1.6153, Ec = 0.3957[ 1000/ 1900]\n",
            "BatLoss = 0.9690, Rc = 0.9357, Ec = 6.6509[ 1500/ 1900]\n",
            "This is epoch 47\n",
            "BatLoss = 0.8937, Rc = 0.8595, Ec = 6.8310[    0/ 1900]\n",
            "BatLoss = 0.8631, Rc = 0.8338, Ec = 5.8446[  500/ 1900]\n",
            "BatLoss = 1.6180, Rc = 1.6151, Ec = 0.5751[ 1000/ 1900]\n",
            "BatLoss = 0.9648, Rc = 0.9321, Ec = 6.5508[ 1500/ 1900]\n",
            "This is epoch 48\n",
            "BatLoss = 0.8890, Rc = 0.8517, Ec = 7.4535[    0/ 1900]\n",
            "BatLoss = 0.8633, Rc = 0.8356, Ec = 5.5463[  500/ 1900]\n",
            "BatLoss = 1.6142, Rc = 1.6125, Ec = 0.3442[ 1000/ 1900]\n",
            "BatLoss = 0.9620, Rc = 0.9305, Ec = 6.3010[ 1500/ 1900]\n",
            "This is epoch 49\n",
            "BatLoss = 0.8833, Rc = 0.8481, Ec = 7.0228[    0/ 1900]\n",
            "BatLoss = 0.8571, Rc = 0.8292, Ec = 5.5694[  500/ 1900]\n",
            "BatLoss = 1.6159, Rc = 1.6134, Ec = 0.4906[ 1000/ 1900]\n",
            "BatLoss = 0.9601, Rc = 0.9290, Ec = 6.2226[ 1500/ 1900]\n",
            "This is epoch 50\n",
            "BatLoss = 0.8807, Rc = 0.8432, Ec = 7.4857[    0/ 1900]\n",
            "BatLoss = 0.8499, Rc = 0.8237, Ec = 5.2350[  500/ 1900]\n",
            "BatLoss = 1.6106, Rc = 1.6075, Ec = 0.6250[ 1000/ 1900]\n",
            "BatLoss = 0.9572, Rc = 0.9276, Ec = 5.9141[ 1500/ 1900]\n",
            "This is epoch 51\n",
            "BatLoss = 0.8753, Rc = 0.8393, Ec = 7.1963[    0/ 1900]\n",
            "BatLoss = 0.8411, Rc = 0.8146, Ec = 5.2842[  500/ 1900]\n",
            "BatLoss = 1.6130, Rc = 1.6100, Ec = 0.5956[ 1000/ 1900]\n",
            "BatLoss = 0.9532, Rc = 0.9243, Ec = 5.7840[ 1500/ 1900]\n",
            "This is epoch 52\n",
            "BatLoss = 0.8721, Rc = 0.8366, Ec = 7.0992[    0/ 1900]\n",
            "BatLoss = 0.8453, Rc = 0.8207, Ec = 4.9278[  500/ 1900]\n",
            "BatLoss = 1.6085, Rc = 1.6056, Ec = 0.5664[ 1000/ 1900]\n",
            "BatLoss = 0.9493, Rc = 0.9218, Ec = 5.4961[ 1500/ 1900]\n",
            "This is epoch 53\n",
            "BatLoss = 0.8655, Rc = 0.8312, Ec = 6.8610[    0/ 1900]\n",
            "BatLoss = 0.8351, Rc = 0.8109, Ec = 4.8383[  500/ 1900]\n",
            "BatLoss = 1.6091, Rc = 1.6061, Ec = 0.6163[ 1000/ 1900]\n",
            "BatLoss = 0.9461, Rc = 0.9185, Ec = 5.5053[ 1500/ 1900]\n",
            "This is epoch 54\n",
            "BatLoss = 0.8643, Rc = 0.8303, Ec = 6.8063[    0/ 1900]\n",
            "BatLoss = 0.8344, Rc = 0.8107, Ec = 4.7346[  500/ 1900]\n",
            "BatLoss = 1.6084, Rc = 1.6034, Ec = 0.9956[ 1000/ 1900]\n",
            "BatLoss = 0.9432, Rc = 0.9172, Ec = 5.1878[ 1500/ 1900]\n",
            "This is epoch 55\n",
            "BatLoss = 0.8568, Rc = 0.8247, Ec = 6.4279[    0/ 1900]\n",
            "BatLoss = 0.8270, Rc = 0.8043, Ec = 4.5213[  500/ 1900]\n",
            "BatLoss = 1.6052, Rc = 1.6023, Ec = 0.5691[ 1000/ 1900]\n",
            "BatLoss = 0.9427, Rc = 0.9164, Ec = 5.2578[ 1500/ 1900]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the trained model in the output folder for future use. "
      ],
      "metadata": {
        "id": "0ZHC1FObH6P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net, output_dir + 'model_pretrain.pt') ## save it in the output folder"
      ],
      "metadata": {
        "id": "YJgRhc7DH3XW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II.  PGRNN Training \n",
        "\n",
        "In this part, we *re-train* the model using observed data. The real data has missing observations, which will be *masked* in the loss calculation. "
      ],
      "metadata": {
        "id": "UiflUZazsA81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "mlyRTEYcshG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data ---------------------------------------------------------------\n",
        "x_full = np.load(data_dir + 'processed_features.npy') # input data\n",
        "x_raw_full = np.load(data_dir + 'features.npy') # raw input data\n",
        "diag_full = np.load(data_dir + 'diag.npy')\n",
        "label = np.load(data_dir + 'Obs_temp.npy') # real observation data\n",
        "mask = np.load(data_dir + 'Obs_mask.npy') # flags of missing values\n",
        "phy_full = np.concatenate((x_raw_full[:,:,:-2], diag_full), axis = 2) #physics variables\n",
        "\n",
        "# train-val-test split\n",
        "## Here we use equal size for train-val-test split\n",
        "## They can be of different sizes by using different n_steps \n",
        "idx_tr, idx_va, idx_te = (4000, 8000, 12000)\n",
        "print(idx_tr, idx_va, idx_te)\n",
        "\n",
        "## training\n",
        "x_tr = x_full[:, :idx_tr]\n",
        "y_tr = label[:, :idx_tr]\n",
        "p_tr = phy_full[:, :idx_tr]\n",
        "m_tr = mask[:, :idx_tr]\n",
        "## validation\n",
        "x_va = x_full[:, idx_tr:idx_va]\n",
        "y_va = label[:, idx_tr:idx_va]\n",
        "p_va = phy_full[:, idx_tr:idx_va]\n",
        "m_va = mask[:, idx_tr:idx_va]\n",
        "\n",
        "\n",
        "## testing\n",
        "x_te = x_full[:, idx_va:idx_te]\n",
        "y_te = label[:, idx_va:idx_te]\n",
        "p_te = phy_full[:, idx_va:idx_te]\n",
        "m_te = mask[:, idx_va:idx_te]\n",
        "\n",
        "# sparsify mask\n",
        "\n",
        "s_perc =0.99 #0.002 #0.2#0.4 #0.6 #0.8\n",
        "## training\n",
        "m_tr = np.reshape(m_tr, (-1, 1))\n",
        "loc_tr = np.random.choice(np.arange(m_tr.shape[0]), replace=False, size=int(m_tr.shape[0] * (1-s_perc)))\n",
        "m_tr[loc_tr, 0] = 0.0\n",
        "## validation\n",
        "m_va = np.reshape(m_va, (-1, 1))\n",
        "loc_va = np.random.choice(np.arange(m_va.shape[0]), replace=False, size=int(m_va.shape[0] * (1-s_perc)))\n",
        "m_va[loc_va, 0] = 0.0\n",
        "## reshaping\n",
        "m_tr = np.reshape(m_tr, (50, -1))\n",
        "m_va = np.reshape(m_va, (50, -1))\n",
        "\n",
        "# create data\n",
        "n_steps = int(idx_tr/npic)\n",
        "x_train = np.zeros([n_depths * N_sec, n_steps, input_size])\n",
        "y_train = np.zeros([n_depths * N_sec, n_steps])\n",
        "p_train = np.zeros([n_depths * N_sec, n_steps, phy_size])\n",
        "m_train = np.zeros([n_depths * N_sec, n_steps])\n",
        "\n",
        "x_val = np.zeros([n_depths * N_sec, n_steps, input_size])\n",
        "y_val = np.zeros([n_depths * N_sec, n_steps])\n",
        "p_val = np.zeros([n_depths * N_sec, n_steps, phy_size])\n",
        "m_val = np.zeros([n_depths * N_sec, n_steps])\n",
        "\n",
        "x_test = np.zeros([n_depths * N_sec, n_steps, input_size])\n",
        "y_test = np.zeros([n_depths * N_sec, n_steps])\n",
        "p_test = np.zeros([n_depths * N_sec, n_steps, phy_size])\n",
        "m_test = np.zeros([n_depths * N_sec, n_steps])\n",
        "\n",
        "for i in range(1, N_sec + 1):\n",
        "    r_1 = int((i - 1) * n_steps / 2) ## stride size = n_steps / 2\n",
        "    r_2 = r_1 + n_steps ## step size = n_step\n",
        "    x_train[(i-1)*n_depths:(i*n_depths)] = x_tr[:, r_1:r_2]\n",
        "    y_train[(i-1)*n_depths:(i*n_depths)] = y_tr[:, r_1:r_2]\n",
        "    p_train[(i-1)*n_depths:(i*n_depths)] = p_tr[:, r_1:r_2]\n",
        "    m_train[(i-1)*n_depths:(i*n_depths)] = m_tr[:, r_1:r_2]\n",
        "    x_val[(i-1)*n_depths:(i*n_depths)] = x_va[:, r_1:r_2]\n",
        "    y_val[(i-1)*n_depths:(i*n_depths)] = y_va[:, r_1:r_2]\n",
        "    p_val[(i-1)*n_depths:(i*n_depths)] = p_va[:, r_1:r_2]\n",
        "    m_val[(i-1)*n_depths:(i*n_depths)] = m_va[:, r_1:r_2]\n",
        "    x_test[(i-1)*n_depths:(i*n_depths)] = x_te[:, r_1:r_2]\n",
        "    y_test[(i-1)*n_depths:(i*n_depths)] = y_te[:, r_1:r_2]\n",
        "    p_test[(i-1)*n_depths:(i*n_depths)] = p_te[:, r_1:r_2]\n",
        "    m_test[(i-1)*n_depths:(i*n_depths)] = m_te[:, r_1:r_2]\n",
        "\n",
        "x_f = np.concatenate((x_train, x_val), axis=0)\n",
        "y_f = np.concatenate((y_train, y_val), axis=0)\n",
        "p_f = np.concatenate((p_train, p_val), axis=0)\n",
        "m_f = np.concatenate((m_train, m_val), axis=0)"
      ],
      "metadata": {
        "id": "3fWb1GK6vH1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c626e406-49f4-4887-b41f-6f4d99e317b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000 8000 12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create data loaders"
      ],
      "metadata": {
        "id": "FqUHyeSdQXTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(x_train), \n",
        "                           torch.from_numpy(y_train), \n",
        "                           torch.from_numpy(m_train),\n",
        "                           torch.from_numpy(p_train))\n",
        "\n",
        "val_data = TensorDataset(torch.from_numpy(x_val), \n",
        "                         torch.from_numpy(y_val), \n",
        "                         torch.from_numpy(m_val),\n",
        "                         torch.from_numpy(p_val))\n",
        "\n",
        "test_data = TensorDataset(torch.from_numpy(x_test), \n",
        "                           torch.from_numpy(y_test), \n",
        "                           torch.from_numpy(m_test),\n",
        "                           torch.from_numpy(p_test))\n",
        "\n",
        "full_data = TensorDataset(torch.from_numpy(x_f), \n",
        "                          torch.from_numpy(y_f), \n",
        "                          torch.from_numpy(m_f),\n",
        "                          torch.from_numpy(p_f))\n",
        "\n",
        "tr_loader = DataLoader(train_data, batch_sampler=batch_sample_generator(len(train_data), batch_size))\n",
        "va_loader = DataLoader(val_data, batch_sampler=batch_sample_generator(len(val_data), batch_size))\n",
        "te_loader = DataLoader(test_data, batch_sampler=batch_sample_generator(len(test_data), batch_size))\n",
        "fu_loader = DataLoader(full_data, batch_sampler=batch_sample_generator(len(full_data), batch_size))"
      ],
      "metadata": {
        "id": "cY8_97uA66gw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with early stopping"
      ],
      "metadata": {
        "id": "TAQiv6zrQbtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bwh4if7ShNf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28eb1001-94a7-4a34-918d-89398e14be73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is epoch 1\n",
            "BatLoss = 3.2248, Rc = 3.1938, Ec = 6.2008[    0/  950]\n",
            "BatLoss = 1.6117, Rc = 1.5062, Ec = 21.0838[  500/  950]\n",
            "The current loss: 1.741180802536159\n",
            "This is epoch 2\n",
            "BatLoss = 1.9622, Rc = 1.8295, Ec = 26.5394[    0/  950]\n",
            "BatLoss = 1.3531, Rc = 1.2428, Ec = 22.0519[  500/  950]\n",
            "The current loss: 1.5428638006758564\n",
            "This is epoch 3\n",
            "BatLoss = 1.6182, Rc = 1.5324, Ec = 17.1558[    0/  950]\n",
            "BatLoss = 1.2858, Rc = 1.1911, Ec = 18.9268[  500/  950]\n",
            "The current loss: 1.424054761733425\n",
            "This is epoch 4\n",
            "BatLoss = 1.5675, Rc = 1.5017, Ec = 13.1495[    0/  950]\n",
            "BatLoss = 1.2294, Rc = 1.1729, Ec = 11.3008[  500/  950]\n",
            "The current loss: 1.433838014217607\n",
            "This is epoch 5\n",
            "BatLoss = 1.5143, Rc = 1.4594, Ec = 10.9644[    0/  950]\n",
            "BatLoss = 1.1744, Rc = 1.1256, Ec = 9.7647[  500/  950]\n",
            "The current loss: 1.4353361403790696\n",
            "This is epoch 6\n",
            "BatLoss = 1.4672, Rc = 1.4267, Ec = 8.1114[    0/  950]\n",
            "BatLoss = 1.1762, Rc = 1.1313, Ec = 8.9671[  500/  950]\n",
            "The current loss: 1.4042916749547893\n",
            "This is epoch 7\n",
            "BatLoss = 1.4740, Rc = 1.4307, Ec = 8.6612[    0/  950]\n",
            "BatLoss = 1.1664, Rc = 1.1257, Ec = 8.1461[  500/  950]\n",
            "The current loss: 1.4090651053147423\n",
            "This is epoch 8\n",
            "BatLoss = 1.4647, Rc = 1.4240, Ec = 8.1378[    0/  950]\n",
            "BatLoss = 1.1444, Rc = 1.1003, Ec = 8.8283[  500/  950]\n",
            "The current loss: 1.4182845619602265\n",
            "This is epoch 9\n",
            "BatLoss = 1.4540, Rc = 1.4142, Ec = 7.9432[    0/  950]\n",
            "BatLoss = 1.1444, Rc = 1.1006, Ec = 8.7514[  500/  950]\n",
            "The current loss: 1.4148974105974266\n",
            "Early stopping!\n",
            "Start to test process.\n"
          ]
        }
      ],
      "source": [
        "# train the model with early stopping\n",
        "## initialize model\n",
        "net = torch.load(output_dir + 'model_pretrain.pt').to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "net, best_epoch = train(net, epochs, optimizer, tr_loader, va_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain with training + validation using best parameters"
      ],
      "metadata": {
        "id": "mTnhbev4Qez9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional: retrain the model using the best #epochs over train+val\n",
        "## initialize model\n",
        "net = torch.load(output_dir + 'model_pretrain.pt').to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "net, _ = train(net, best_epoch, optimizer, fu_loader, fu_loader, early_stopping=False)\n",
        "## save the model\n",
        "torch.save(net, output_dir + 'model_finetune.pt') ## we save it in the data folder for simplicity, you can choose to save it in a different folder"
      ],
      "metadata": {
        "id": "xlsgetyVUevG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca6bf04-840d-4d09-afc9-4ec8ef363079"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is epoch 1\n",
            "BatLoss = 3.2248, Rc = 3.1938, Ec = 6.2008[    0/ 1900]\n",
            "BatLoss = 1.6117, Rc = 1.5062, Ec = 21.0838[  500/ 1900]\n",
            "BatLoss = 1.7743, Rc = 1.6572, Ec = 23.4201[ 1000/ 1900]\n",
            "BatLoss = 1.4268, Rc = 1.3142, Ec = 22.5071[ 1500/ 1900]\n",
            "This is epoch 2\n",
            "BatLoss = 1.7758, Rc = 1.6938, Ec = 16.3833[    0/ 1900]\n",
            "BatLoss = 1.3416, Rc = 1.2485, Ec = 18.6059[  500/ 1900]\n",
            "BatLoss = 1.5127, Rc = 1.4269, Ec = 17.1616[ 1000/ 1900]\n",
            "BatLoss = 1.2057, Rc = 1.1131, Ec = 18.5177[ 1500/ 1900]\n",
            "This is epoch 3\n",
            "BatLoss = 1.7302, Rc = 1.6646, Ec = 13.1176[    0/ 1900]\n",
            "BatLoss = 1.2567, Rc = 1.2025, Ec = 10.8379[  500/ 1900]\n",
            "BatLoss = 1.5517, Rc = 1.4864, Ec = 13.0614[ 1000/ 1900]\n",
            "BatLoss = 1.2368, Rc = 1.1370, Ec = 19.9513[ 1500/ 1900]\n",
            "This is epoch 4\n",
            "BatLoss = 1.6970, Rc = 1.6371, Ec = 11.9929[    0/ 1900]\n",
            "BatLoss = 1.2620, Rc = 1.2047, Ec = 11.4622[  500/ 1900]\n",
            "BatLoss = 1.5553, Rc = 1.4807, Ec = 14.9229[ 1000/ 1900]\n",
            "BatLoss = 1.2457, Rc = 1.1352, Ec = 22.1014[ 1500/ 1900]\n",
            "This is epoch 5\n",
            "BatLoss = 1.7301, Rc = 1.6520, Ec = 15.6178[    0/ 1900]\n",
            "BatLoss = 1.2200, Rc = 1.1639, Ec = 11.2126[  500/ 1900]\n",
            "BatLoss = 1.5467, Rc = 1.4747, Ec = 14.3880[ 1000/ 1900]\n",
            "BatLoss = 1.2713, Rc = 1.1551, Ec = 23.2390[ 1500/ 1900]\n",
            "This is epoch 6\n",
            "BatLoss = 1.7421, Rc = 1.6639, Ec = 15.6325[    0/ 1900]\n",
            "BatLoss = 1.2183, Rc = 1.1538, Ec = 12.8857[  500/ 1900]\n",
            "BatLoss = 1.5093, Rc = 1.4497, Ec = 11.9183[ 1000/ 1900]\n",
            "BatLoss = 1.3011, Rc = 1.1810, Ec = 24.0136[ 1500/ 1900]\n",
            "This is epoch 7\n",
            "BatLoss = 1.6854, Rc = 1.6276, Ec = 11.5584[    0/ 1900]\n",
            "BatLoss = 1.1814, Rc = 1.1271, Ec = 10.8723[  500/ 1900]\n",
            "BatLoss = 1.4930, Rc = 1.4409, Ec = 10.4149[ 1000/ 1900]\n",
            "BatLoss = 1.2159, Rc = 1.1073, Ec = 21.7064[ 1500/ 1900]\n",
            "This is epoch 8\n",
            "BatLoss = 1.6910, Rc = 1.6363, Ec = 10.9365[    0/ 1900]\n",
            "BatLoss = 1.1381, Rc = 1.0935, Ec = 8.9064[  500/ 1900]\n",
            "BatLoss = 1.4989, Rc = 1.4406, Ec = 11.6755[ 1000/ 1900]\n",
            "BatLoss = 1.1697, Rc = 1.0668, Ec = 20.5852[ 1500/ 1900]\n",
            "This is epoch 9\n",
            "BatLoss = 1.6911, Rc = 1.6308, Ec = 12.0463[    0/ 1900]\n",
            "BatLoss = 1.1047, Rc = 1.0750, Ec = 5.9380[  500/ 1900]\n",
            "BatLoss = 1.4794, Rc = 1.4256, Ec = 10.7651[ 1000/ 1900]\n",
            "BatLoss = 1.1203, Rc = 1.0184, Ec = 20.3827[ 1500/ 1900]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation over test set"
      ],
      "metadata": {
        "id": "UfKsrBezQnKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the performance over test set\n",
        "net = torch.load(output_dir + 'model_finetune.pt').to(device)\n",
        "net.eval()\n",
        "print('Test loss is:', val_test(net, te_loader))"
      ],
      "metadata": {
        "id": "tcEvgboZUeqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da19813a-e0bb-4416-fbb4-ec50050a0f11"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss is: 1.3718351454682318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if there is no finetune\n",
        "net = torch.load(output_dir + 'model_pretrain.pt').to(device)\n",
        "net.eval()\n",
        "print('Test loss is:', val_test(net, te_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2iqRtHNUTyX",
        "outputId": "be7cb965-d707-465c-f71e-9c57713a496b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss is: 2.3579796069253893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations\n",
        "\n",
        "In the following we create a few visualization of the data and model outputs to demonstrate the performance of the models. "
      ],
      "metadata": {
        "id": "PzgSNLcIBVmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dates information\n",
        "dates_arr = np.load(data_dir + 'dates.npy')\n",
        "dates_d = dates_arr.astype('M8[D]') ## transform the dates to date\n",
        "dates_m = dates_arr.astype('M8[M]') ## transform the dates to date\n",
        "dates_y = dates_arr.astype('M8[Y]') ## transform the dates to year\n",
        "# load features\n",
        "x_raw = np.load(data_dir + 'features.npy')\n",
        "x_proc = np.load(data_dir + 'processed_features.npy')\n",
        "# load the labels\n",
        "label = np.load(data_dir + 'Obs_temp.npy')\n",
        "print('Shape of labels:', label.shape)\n",
        "# load the weight\n",
        "weight = np.load(data_dir + 'Obs_mask.npy')\n",
        "print('Shape of weight:', weight.shape)\n",
        "print('Mean of weight:', np.mean(weight)) ## data is very sparse, 2% of the data are not nan\n",
        "# filter the test set\n",
        "label_test = label[:, idx_va:idx_te]\n",
        "weight_test = weight[:, idx_va:idx_te]\n",
        "dates_test_d = dates_d[idx_va:idx_te]\n",
        "dates_test_m = dates_m[idx_va:idx_te]\n",
        "dates_test_y = dates_y[idx_va:idx_te].astype(str)\n",
        "print('Years available:', np.unique(dates_test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLwxhYq_hKvZ",
        "outputId": "de2b8732-6f2a-4457-c550-3e46b65a8a9f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labels: (50, 12691)\n",
            "Shape of weight: (50, 12691)\n",
            "Mean of weight: 0.020735954613505633\n",
            "Years available: ['2002' '2003' '2004' '2005' '2006' '2007' '2008' '2009' '2010' '2011'\n",
            " '2012' '2013']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the weight\n",
        "## black: data, white: zero-weight (NA value?)\n",
        "year_choose = '2005'\n",
        "idx = np.where(dates_test_y == year_choose)[0]\n",
        "n_row = label_test.shape[0]\n",
        "n_col = len(idx)\n",
        "## make the figure\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "im = ax.imshow(weight_test[:, idx] == 0, aspect='auto', cmap=plt.cm.gray)\n",
        "x_choose = np.array(np.linspace(0, 1, 8) * (n_col-1), dtype=int)\n",
        "ax.set_xticks(x_choose)\n",
        "ax.set_xticklabels(dates_test_d[idx][x_choose], rotation=30, fontdict={'horizontalalignment': 'center'})\n",
        "ax.set_xlabel('Date')\n",
        "y_choose = np.array(np.linspace(0, 1, 8) * (n_row-1), dtype=int)\n",
        "ax.set_yticks(y_choose)\n",
        "ax.set_yticklabels(np.around(np.linspace(x_raw[0, 0, 1], x_raw[-1, -1, 1], n_row)[y_choose], decimals=1))\n",
        "ax.set_ylabel('Depth')\n",
        "ax.set_title('Sample Weight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "mCXvzzlmeXqv",
        "outputId": "d95b023a-a287-41be-dc7d-b30f79f26bf2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the true values\n",
        "import matplotlib\n",
        "weight_mat = weight_test[:, idx]\n",
        "weight_sum = np.sum(weight_mat, axis=0)\n",
        "idx_dt = np.where(weight_sum > 0)[0]\n",
        "n_col = len(idx_dt)\n",
        "label_mat = label_test[:, idx].copy()\n",
        "label_mat[weight_test[:, idx] == 0] = np.nan\n",
        "## make the figure\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "cmap = matplotlib.cm.coolwarm\n",
        "cmap.set_bad('black') ## we set the nan values to black, you can change it to other colors\n",
        "im = ax.imshow(label_mat[:, idx_dt], aspect='auto', cmap=cmap)\n",
        "ax.set_xticks(np.arange(n_col))\n",
        "ax.set_xticklabels(dates_test_d[idx][idx_dt], rotation=30, fontdict={'horizontalalignment': 'center'})\n",
        "ax.set_xlabel('Date')\n",
        "y_choose = np.array(np.linspace(0, 1, 8) * (n_row-1), dtype=int)\n",
        "ax.set_yticks(y_choose)\n",
        "ax.set_yticklabels(np.around(np.linspace(x_raw[0, 0, 1], x_raw[-1, -1, 1], n_row)[y_choose], decimals=1))\n",
        "ax.set_ylabel('Depth')\n",
        "ax.set_title('True Values of Year ' + str(year_choose) )\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "isIojKG8jqdh",
        "outputId": "7078207c-8189-4177-d709-e64a43cee921"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the predictions\n",
        "net = torch.load(data_dir + 'model_finetune.pt').to(device)\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    x = torch.from_numpy(x_test).to(device).float()\n",
        "    pred_mat = net(x).cpu().numpy()\n",
        "\n",
        "label_pred = np.zeros(label_test.shape)\n",
        "for i in range(1, N_sec + 1):\n",
        "    r_1 = int((i - 1) * n_steps / 2) ## stride size = n_steps / 2\n",
        "    r_2 = r_1 + n_steps ## step size = n_step\n",
        "    label_pred[:, r_1:r_2] = pred_mat[(i-1)*n_depths:(i*n_depths)].copy()\n",
        "\n",
        "vis_mat = label_pred[:, idx].copy()\n",
        "vis_mat[weight_test[:, idx] == 0] = np.nan\n",
        "## make the figure\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "cmap = matplotlib.cm.coolwarm\n",
        "cmap.set_bad('black') ## we set the nan values to black, you can change it to other colors\n",
        "im = ax.imshow(vis_mat[:, idx_dt], aspect='auto', cmap=cmap)\n",
        "ax.set_xticks(np.arange(n_col))\n",
        "ax.set_xticklabels(dates_test_d[idx][idx_dt], rotation=30, fontdict={'horizontalalignment': 'center'})\n",
        "ax.set_xlabel('Date')\n",
        "y_choose = np.array(np.linspace(0, 1, 8) * (n_row-1), dtype=int)\n",
        "ax.set_yticks(y_choose)\n",
        "ax.set_yticklabels(np.around(np.linspace(x_raw[0, 0, 1], x_raw[-1, -1, 1], n_row)[y_choose], decimals=1))\n",
        "ax.set_ylabel('Depth')\n",
        "ax.set_title('Predicted Values of Year ' + str(year_choose) )\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "_dy_8n0xno4n",
        "outputId": "1d0945b8-1a45-41af-bd31-bafa096520ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-25e1f83a145f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualize the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model_finetune.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/LEAP Colab Notebooks/Project 2/Data/model_finetune.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the residuals = true labels - predicted values\n",
        "err_mat = label_test - label_pred\n",
        "vis_mat = err_mat[:, idx].copy()\n",
        "vis_mat[weight_test[:, idx] == 0] = np.nan\n",
        "## make the figure\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "cmap = matplotlib.cm.coolwarm\n",
        "cmap.set_bad('black') ## we set the nan values to black, you can change it to other colors\n",
        "im = ax.imshow(vis_mat[:, idx_dt], aspect='auto', cmap=cmap)\n",
        "ax.set_xticks(np.arange(n_col))\n",
        "ax.set_xticklabels(dates_test_d[idx][idx_dt], rotation=30, fontdict={'horizontalalignment': 'center'})\n",
        "ax.set_xlabel('Date')\n",
        "y_choose = np.array(np.linspace(0, 1, 8) * (n_row-1), dtype=int)\n",
        "ax.set_yticks(y_choose)\n",
        "ax.set_yticklabels(np.around(np.linspace(x_raw[0, 0, 1], x_raw[-1, -1, 1], n_row)[y_choose], decimals=1))\n",
        "ax.set_ylabel('Depth')\n",
        "ax.set_title('Residuals of Year ' + str(year_choose) )\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VE7oj4nooC_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part II. visualizations for comparing pretrained model and fine-tuned model\n",
        "net = torch.load(data_dir + 'model_pretrain.pt').to(device)\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    x = torch.from_numpy(x_test).to(device).float()\n",
        "    pred_pret = net(x).cpu().numpy()\n",
        "\n",
        "label_pret = np.zeros(label_test.shape)\n",
        "for i in range(1, N_sec + 1):\n",
        "    r_1 = int((i - 1) * n_steps / 2) ## stride size = n_steps / 2\n",
        "    r_2 = r_1 + n_steps ## step size = n_step\n",
        "    label_pret[:, r_1:r_2] = pred_pret[(i-1)*n_depths:(i*n_depths)].copy()\n",
        "err_pret = label_test - label_pret\n",
        "\n",
        "net = torch.load(data_dir + 'model_finetune.pt').to(device)\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    x = torch.from_numpy(x_test).to(device).float()\n",
        "    pred_fine = net(x).cpu().numpy()\n",
        "\n",
        "label_fine = np.zeros(label_test.shape)\n",
        "for i in range(1, N_sec + 1):\n",
        "    r_1 = int((i - 1) * n_steps / 2) ## stride size = n_steps / 2\n",
        "    r_2 = r_1 + n_steps ## step size = n_step\n",
        "    label_fine[:, r_1:r_2] = pred_fine[(i-1)*n_depths:(i*n_depths)].copy()\n",
        "err_fine = label_test - label_fine"
      ],
      "metadata": {
        "id": "-rMf4FaySH5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizatin along different months\n",
        "## keep the month information and transform to int\n",
        "dates_test_m_int = np.array([str(i)[-2:] for i in dates_test_m]).astype(int)\n",
        "RMSE_pret = []\n",
        "RMSE_fine = []\n",
        "for i in range(1, 13):\n",
        "    weight_m = weight_test[:, dates_test_m_int == i]\n",
        "    if weight_m.sum() > 0:\n",
        "        loc_temp = np.where(np.sum(weight_m, axis=1) > 0)[0]\n",
        "        err_pret_m = err_pret[:, dates_test_m_int == i]\n",
        "        RMSE_m = np.sqrt(np.sum(weight_m * err_pret_m ** 2, axis=1)[loc_temp] / np.sum(weight_m, axis=1)[loc_temp])\n",
        "        RMSE_pret.append(list(RMSE_m))\n",
        "\n",
        "        err_fine_m = err_fine[:, dates_test_m_int == i]\n",
        "        RMSE_m = np.sqrt(np.sum(weight_m * err_fine_m ** 2, axis=1)[loc_temp] / np.sum(weight_m, axis=1)[loc_temp])\n",
        "        RMSE_fine.append(list(RMSE_m))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bpl = plt.boxplot(RMSE_pret, positions=6*np.arange(12) - 1, widths=1)\n",
        "bpr = plt.boxplot(RMSE_fine, positions=6*np.arange(12) + 1, widths=1)\n",
        "\n",
        "ticks = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "def define_box_properties(plot_name, color_code, label):\n",
        "\tfor k, v in plot_name.items():\n",
        "\t\tplt.setp(plot_name.get(k), color=color_code)\n",
        "\t\t\n",
        "\t# use plot function to draw a small line to name the legend.\n",
        "\tplt.plot([], c=color_code, label=label)\n",
        "\tplt.legend()\n",
        "\n",
        "\n",
        "# setting colors for each groups\n",
        "define_box_properties(bpl, '#D7191C', 'Pretrained')\n",
        "define_box_properties(bpr, '#2C7BB6', 'Finetuned')\n",
        "plt.legend()\n",
        "# set the x label values\n",
        "plt.xticks(np.arange(0, len(ticks) * 6, 6), ticks)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlim(-3, len(ticks) * 6)\n",
        "plt.title('RMSE by Month')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Okgs5fubSHy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import jit\n",
        "# visualizatin along different depths\n",
        "depth_vec = x_raw[:, 0, 1]\n",
        "RMSE_pret = []\n",
        "RMSE_fine = []\n",
        "for i in range(len(depth_vec)):\n",
        "    RMSE_pret_temp = []\n",
        "    RMSE_fine_temp = []\n",
        "    for j in range(12):\n",
        "        weight_m = weight_test[i, dates_test_m_int == j]\n",
        "        if weight_m.sum() > 0:\n",
        "            err_pret_m = err_pret[i, dates_test_m_int == j]\n",
        "            RMSE_m = np.sqrt(np.sum(weight_m * err_pret_m ** 2) / np.sum(weight_m))\n",
        "            RMSE_pret_temp.append(RMSE_m)\n",
        "\n",
        "            err_fine_m = err_fine[i, dates_test_m_int == j]\n",
        "            RMSE_m = np.sqrt(np.sum(weight_m * err_fine_m ** 2) / np.sum(weight_m))\n",
        "            RMSE_fine_temp.append(RMSE_m)\n",
        "    RMSE_pret.append(RMSE_pret_temp)\n",
        "    RMSE_fine.append(RMSE_fine_temp)\n",
        "\n",
        "plt.figure(figsize=(25, 5))\n",
        "\n",
        "bpl = plt.boxplot(RMSE_pret, positions=6*np.arange(len(depth_vec)) - 1, widths=1)\n",
        "bpr = plt.boxplot(RMSE_fine, positions=6*np.arange(len(depth_vec)) + 1, widths=1)\n",
        "\n",
        "ticks = np.arange(50) / 2\n",
        "\n",
        "# setting colors for each groups\n",
        "define_box_properties(bpl, '#D7191C', 'Pretrained')\n",
        "define_box_properties(bpr, '#2C7BB6', 'Finetuned')\n",
        "plt.legend()\n",
        "# set the x label values\n",
        "plt.xticks(np.arange(0, len(depth_vec) * 6, 6), ticks)\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlim(-3, len(depth_vec) * 6)\n",
        "plt.title('RMSE by Depth')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7rSLiaAHE-if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "+ Read, J. S., Jia, X., Willard, J., Appling, A. P., Zwart, J. A., Oliver, S. K., ... & Kumar, V. (2019). Processguided deep learning predictions of lake water temperature. [Water Resources Research, 55(11), 9173-9190](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019WR024922).\n",
        "+ Jia, X., Willard, J., Karpatne, A., Read, J., Zwart, J., Steinbach, M., & Kumar, V. (2019, May). Physics guided RNNs for modeling dynamical systems: A case study in simulating lake temperature profiles. [In Proceedings of the 2019 SIAM International Conference on Data Mining (pp. 558-566). Society for Industrial and Applied Mathematics](https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.63).\n",
        "+ Jia, X., Willard, J., Karpatne, A., Read, J. S., Zwart, J. A., Steinbach, M., & Kumar, V. (2021). Physics-guided machine learning for scientific discovery: An application in simulating lake temperature profiles. [ACM/IMS Transactions on Data Science, 2(3), 1-26](https://dl.acm.org/doi/abs/10.1145/3447814)."
      ],
      "metadata": {
        "id": "EFhKD7ZMpRqP"
      }
    }
  ]
}